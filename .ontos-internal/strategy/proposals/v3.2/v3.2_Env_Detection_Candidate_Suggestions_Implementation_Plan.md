---
id: v3_2_env_detection_candidate_suggestions_implementation_plan
type: strategy
status: approved
depends_on: [v3_2_environment_manifest_detection_proposal, v3_2_candidate_suggestions_exploration]
concepts: [environment, onboarding, validation, suggestions, implementation]
---

# v3.2.0 Implementation Plan: Environment Detection + Candidate Suggestions

**Version:** 1.1
**Author:** Chief Architect (Claude Opus 4.5)
**Date:** 2026-01-29
**Status:** Approved — Ready for Implementation

**v1.1 Changes:** Addressed B1 (tomli fallback), B2 (parse warnings), B3 (--force flag), NB3 (specific exceptions), NB6 (deterministic sort). See `v3.2_Chief_Architect_Response.md`.

---

## Executive Summary

This plan combines two v3.2 features into a single implementation:

| Feature | Purpose | Complexity |
|---------|---------|------------|
| **Environment Manifest Detection** | New `ontos env` command to detect and document project environment manifests | Medium |
| **Candidate Suggestions** | Improve broken `depends_on` errors with smart "Did you mean?" suggestions | Low |

**Combined Scope:**
- 2 new modules
- 5-6 modified files
- ~400-500 lines of new code
- No new dependencies (stdlib only)

**Deferred to v3.2.1:**
- `ontos doctor --fix` (auto-repair mode)
- `ontos init --env` integration

---

## Scope Definition

### In Scope (v3.2.0)

| Feature | Component | Description |
|---------|-----------|-------------|
| Env Detection | `ontos env` command | Detect manifests, display summary |
| Env Detection | `ontos env --write` | Generate `.ontos/environment.md` |
| Env Detection | `ontos env --format json` | JSON output for tooling |
| Env Detection | Doctor integration | Add `check_environment_manifests()` |
| Suggestions | `suggest_candidates_for_broken_ref()` | Substring, alias, and Levenshtein matching |
| Suggestions | Graph integration | Enrich `fix_suggestion` field with candidates |
| Suggestions | Map output | Display candidates in context map errors |

### Out of Scope (v3.2.1+)

| Feature | Reason |
|---------|--------|
| `ontos doctor --fix` | Auto-repair adds complexity; ship detection first |
| `ontos init --env` | Projects may not have manifests at init time |
| `ontos env --check` | Requires comparing installed vs. documented (invasive) |
| Monorepo support | Needs scoping rules; defer to feedback |
| `devcontainer.json`, `flake.nix`, `Dockerfile` | Phase 2 manifest types |

---

## Feature 1: Environment Manifest Detection

### 1.1 New File: `ontos/commands/env.py`

```python
"""Environment manifest detection command."""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import json
import re
from datetime import datetime, timezone

# Python 3.11+ has tomllib in stdlib; fall back to tomli for 3.9/3.10
try:
    import tomllib
except ImportError:
    import tomli as tomllib


# =============================================================================
# Data Structures
# =============================================================================

@dataclass
class ManifestInfo:
    """Information about a detected environment manifest."""
    path: Path
    manifest_type: str  # python_deps, system_deps, runtime_versions, npm_deps
    package_manager: Optional[str] = None
    lock_file: Optional[Path] = None
    lock_present: bool = False
    dependency_count: Dict[str, int] = field(default_factory=dict)
    details: Dict = field(default_factory=dict)
    bootstrap_command: str = ""


@dataclass
class EnvResult:
    """Result of environment detection."""
    manifests: List[ManifestInfo] = field(default_factory=list)
    not_detected: List[Dict] = field(default_factory=list)
    onboarding_steps: List[str] = field(default_factory=list)
    parse_warnings: List[str] = field(default_factory=list)  # v1.1: Surface parse failures
    generated_at: str = ""


@dataclass
class EnvOptions:
    """Configuration for env command."""
    path: Path = field(default_factory=Path.cwd)
    write: bool = False
    force: bool = False  # v1.1: Required to overwrite existing environment.md
    format: str = "text"  # text, json
    quiet: bool = False


# =============================================================================
# Manifest Detection
# =============================================================================

MANIFEST_PATTERNS = {
    "pyproject.toml": {
        "type": "python_deps",
        "lock_files": ["poetry.lock", "pdm.lock", "uv.lock"],
        "bootstrap": "poetry install",  # Default, refined during parse
    },
    "requirements.txt": {
        "type": "python_deps",
        "lock_files": [],
        "bootstrap": "pip install -r requirements.txt",
    },
    "Brewfile": {
        "type": "system_deps",
        "lock_files": ["Brewfile.lock.json"],
        "bootstrap": "brew bundle",
    },
    ".tool-versions": {
        "type": "runtime_versions",
        "lock_files": [],
        "bootstrap": "asdf install",
    },
    "package.json": {
        "type": "npm_deps",
        "lock_files": ["package-lock.json", "yarn.lock", "pnpm-lock.yaml"],
        "bootstrap": "npm install",  # Refined based on lock file
    },
    ".nvmrc": {
        "type": "runtime_versions",
        "lock_files": [],
        "bootstrap": "nvm install",
    },
    ".python-version": {
        "type": "runtime_versions",
        "lock_files": [],
        "bootstrap": "pyenv install",
    },
}


def detect_manifests(workspace: Path) -> Tuple[List[ManifestInfo], List[str]]:
    """Scan workspace for known manifest files.

    Returns:
        Tuple of (manifests, parse_warnings)
    """
    manifests = []
    warnings = []

    # Note: Lock file priority order is poetry > pdm > uv (first match wins)
    for filename, spec in MANIFEST_PATTERNS.items():
        manifest_path = workspace / filename
        if manifest_path.exists():
            info = ManifestInfo(
                path=manifest_path,
                manifest_type=spec["type"],
                bootstrap_command=spec["bootstrap"],
            )

            # Check for lock files
            for lock_name in spec["lock_files"]:
                lock_path = workspace / lock_name
                if lock_path.exists():
                    info.lock_file = lock_path
                    info.lock_present = True
                    # Refine bootstrap command based on lock file
                    if lock_name == "yarn.lock":
                        info.bootstrap_command = "yarn install"
                        info.package_manager = "yarn"
                    elif lock_name == "pnpm-lock.yaml":
                        info.bootstrap_command = "pnpm install"
                        info.package_manager = "pnpm"
                    break

            # Parse manifest for details; collect any warnings
            warning = _parse_manifest(info)
            if warning:
                warnings.append(warning)
            manifests.append(info)

    return manifests, warnings


def _parse_manifest(info: ManifestInfo) -> Optional[str]:
    """Parse manifest file to extract details.

    Returns:
        Warning message if parsing failed, None on success.
    """
    try:
        if info.path.name == "pyproject.toml":
            _parse_pyproject(info)
        elif info.path.name == "Brewfile":
            _parse_brewfile(info)
        elif info.path.name == ".tool-versions":
            _parse_tool_versions(info)
        elif info.path.name == "package.json":
            _parse_package_json(info)
        elif info.path.name in (".nvmrc", ".python-version"):
            _parse_single_version(info)
        elif info.path.name == "requirements.txt":
            _parse_requirements(info)
        return None
    except (
        tomllib.TOMLDecodeError,
        json.JSONDecodeError,
        FileNotFoundError,
        PermissionError,
        UnicodeDecodeError,
    ) as e:
        # v1.1: Return warning instead of silent failure
        return f"{info.path.name}: parse failed ({type(e).__name__})"
    except Exception as e:
        # Catch-all for unexpected errors
        return f"{info.path.name}: parse failed ({type(e).__name__})"


def _parse_pyproject(info: ManifestInfo) -> None:
    """Parse pyproject.toml for dependency info."""
    content = info.path.read_text()
    data = tomllib.loads(content)

    # Detect package manager
    if "tool" in data:
        if "poetry" in data["tool"]:
            info.package_manager = "poetry"
            info.bootstrap_command = "poetry install"
            deps = data["tool"]["poetry"].get("dependencies", {})
            dev_deps = data["tool"]["poetry"].get("dev-dependencies", {})
            # Exclude python itself from count
            runtime = len([k for k in deps if k != "python"])
            info.dependency_count = {"runtime": runtime, "dev": len(dev_deps)}
        elif "pdm" in data["tool"]:
            info.package_manager = "pdm"
            info.bootstrap_command = "pdm install"
        elif "uv" in data["tool"]:
            info.package_manager = "uv"
            info.bootstrap_command = "uv sync"

    # PEP 621 dependencies
    if "project" in data:
        deps = data["project"].get("dependencies", [])
        optional = data["project"].get("optional-dependencies", {})
        dev_count = len(optional.get("dev", []))
        info.dependency_count = {"runtime": len(deps), "dev": dev_count}


def _parse_brewfile(info: ManifestInfo) -> None:
    """Parse Brewfile for formulae and casks."""
    content = info.path.read_text()
    formulae = re.findall(r'^brew\s+["\']([^"\']+)', content, re.MULTILINE)
    casks = re.findall(r'^cask\s+["\']([^"\']+)', content, re.MULTILINE)
    mas_apps = re.findall(r'^mas\s+["\']([^"\']+)', content, re.MULTILINE)

    info.details = {
        "formulae": formulae,
        "casks": casks,
        "mas_apps": mas_apps,
    }
    info.dependency_count = {
        "formulae": len(formulae),
        "casks": len(casks),
        "mas_apps": len(mas_apps),
    }


def _parse_tool_versions(info: ManifestInfo) -> None:
    """Parse .tool-versions for runtime versions."""
    content = info.path.read_text()
    runtimes = {}
    for line in content.strip().split("\n"):
        if line.strip() and not line.startswith("#"):
            parts = line.split()
            if len(parts) >= 2:
                runtimes[parts[0]] = parts[1]
    info.details = {"runtimes": runtimes}


def _parse_package_json(info: ManifestInfo) -> None:
    """Parse package.json for dependency info."""
    content = info.path.read_text()
    data = json.loads(content)

    deps = data.get("dependencies", {})
    dev_deps = data.get("devDependencies", {})
    info.dependency_count = {"runtime": len(deps), "dev": len(dev_deps)}

    # Detect package manager from packageManager field
    if "packageManager" in data:
        pm = data["packageManager"].split("@")[0]
        info.package_manager = pm
        info.bootstrap_command = f"{pm} install"


def _parse_single_version(info: ManifestInfo) -> None:
    """Parse single-version files (.nvmrc, .python-version)."""
    version = info.path.read_text().strip()
    info.details = {"version": version}


def _parse_requirements(info: ManifestInfo) -> None:
    """Parse requirements.txt for line count."""
    content = info.path.read_text()
    lines = [l for l in content.split("\n") if l.strip() and not l.startswith("#")]
    info.dependency_count = {"packages": len(lines)}


# =============================================================================
# Onboarding Step Generation
# =============================================================================

# Bootstrap command priority order
BOOTSTRAP_ORDER = [
    "system_deps",      # Install system tools first (Brewfile)
    "runtime_versions", # Then language runtimes (.tool-versions)
    "python_deps",      # Then Python deps
    "npm_deps",         # Then npm deps
]


def generate_onboarding(manifests: List[ManifestInfo]) -> List[str]:
    """Generate ordered bootstrap commands."""
    by_type = {}
    for m in manifests:
        by_type.setdefault(m.manifest_type, []).append(m)

    steps = []
    for manifest_type in BOOTSTRAP_ORDER:
        for m in by_type.get(manifest_type, []):
            if m.bootstrap_command:
                steps.append(m.bootstrap_command)

    return steps


# =============================================================================
# Output Formatting
# =============================================================================

def format_text_output(result: EnvResult) -> str:
    """Format result as human-readable text."""
    lines = ["Environment Manifest Detection", ""]

    if result.manifests:
        lines.append("Detected Manifests:")
        for i, m in enumerate(result.manifests):
            prefix = "├──" if i < len(result.manifests) - 1 else "└──"
            type_label = _type_label(m.manifest_type)
            lines.append(f"  {prefix} {m.path.name} ({type_label})")

            # Details line
            detail_parts = []
            if m.dependency_count:
                counts = ", ".join(f"{v} {k}" for k, v in m.dependency_count.items())
                detail_parts.append(counts)
            if m.lock_present:
                detail_parts.append(f"{m.lock_file.name} present")
            if m.details.get("runtimes"):
                rt = m.details["runtimes"]
                detail_parts.append(", ".join(f"{k} {v}" for k, v in rt.items()))

            if detail_parts:
                detail_prefix = "│   └──" if i < len(result.manifests) - 1 else "    └──"
                lines.append(f"  {detail_prefix} {'; '.join(detail_parts)}")
    else:
        lines.append("No environment manifests detected.")

    if result.not_detected:
        lines.append("")
        lines.append("Not Detected:")
        for nd in result.not_detected:
            lines.append(f"  └── {nd['expected_file']} ({nd.get('reason', 'not found')})")

    if result.onboarding_steps:
        lines.append("")
        lines.append("Onboarding Steps:")
        for i, step in enumerate(result.onboarding_steps, 1):
            lines.append(f"  {i}. {step}")

    # v1.1: Display parse warnings so users know when detection is incomplete
    if result.parse_warnings:
        lines.append("")
        lines.append("Parse Warnings:")
        for warning in result.parse_warnings:
            lines.append(f"  - {warning}")

    lines.append("")
    lines.append("Run 'ontos env --write' to save to .ontos/environment.md")

    return "\n".join(lines)


def _type_label(manifest_type: str) -> str:
    """Convert manifest type to human label."""
    labels = {
        "python_deps": "Python dependencies",
        "system_deps": "System dependencies",
        "runtime_versions": "Runtime versions",
        "npm_deps": "npm dependencies",
    }
    return labels.get(manifest_type, manifest_type)


def format_json_output(result: EnvResult) -> Dict:
    """Format result as JSON-serializable dict."""
    return {
        "$schema": "ontos-env-v1",
        "generated_at": result.generated_at,
        "manifests": [
            {
                "path": str(m.path.name),
                "type": m.manifest_type,
                "package_manager": m.package_manager,
                "lock_file": str(m.lock_file.name) if m.lock_file else None,
                "lock_present": m.lock_present,
                "dependency_count": m.dependency_count,
                "details": m.details,
                "bootstrap_command": m.bootstrap_command,
            }
            for m in result.manifests
        ],
        "not_detected": result.not_detected,
        "onboarding_steps": result.onboarding_steps,
        "parse_warnings": result.parse_warnings,  # v1.1: Include warnings in JSON
    }


def generate_environment_md(result: EnvResult, workspace: Path) -> str:
    """Generate .ontos/environment.md content."""
    lines = [
        "# Environment Setup",
        "",
        "> Auto-generated by `ontos env`. Do not edit manually.",
        f"> Regenerate with: `ontos env --write`",
        f"> Last updated: {result.generated_at}",
        "",
        "---",
        "",
        "## Detected Manifests",
        "",
        "| Manifest | Type | Purpose | Bootstrap Command |",
        "|----------|------|---------|-------------------|",
    ]

    for m in result.manifests:
        rel_path = f"../{m.path.name}"
        lines.append(
            f"| [{m.path.name}]({rel_path}) | {_type_label(m.manifest_type)} | "
            f"{m.package_manager or 'N/A'} | `{m.bootstrap_command}` |"
        )

    lines.extend([
        "",
        "---",
        "",
        "## Quick Start",
        "",
        "New contributors should run these commands in order:",
        "",
        "```bash",
    ])

    for i, step in enumerate(result.onboarding_steps, 1):
        lines.append(f"# {i}. {_step_comment(step)}")
        lines.append(step)
        lines.append("")

    lines.extend([
        "```",
        "",
        "---",
        "",
        "## Manifest Details",
        "",
    ])

    for m in result.manifests:
        lines.append(f"### {m.path.name}")
        lines.append("")
        lines.append(f"**Location:** `{m.path.name}`")
        if m.package_manager:
            lines.append(f"**Package Manager:** {m.package_manager}")
        if m.dependency_count:
            counts = ", ".join(f"{v} {k}" for k, v in m.dependency_count.items())
            lines.append(f"**Dependencies:** {counts}")
        if m.lock_present:
            lines.append(f"**Lock File:** Present ({m.lock_file.name})")
        if m.details.get("runtimes"):
            lines.append("**Runtimes:**")
            for rt, ver in m.details["runtimes"].items():
                lines.append(f"- {rt}: {ver}")
        if m.details.get("formulae"):
            lines.append(f"**Formulae:** {', '.join(m.details['formulae'])}")
        if m.details.get("casks"):
            lines.append(f"**Casks:** {', '.join(m.details['casks'])}")
        lines.append("")

    return "\n".join(lines)


def _step_comment(step: str) -> str:
    """Generate comment for bootstrap step."""
    if "brew" in step:
        return "System dependencies (macOS)"
    elif "asdf" in step or "nvm" in step:
        return "Runtime versions"
    elif "poetry" in step or "pip" in step or "pdm" in step or "uv" in step:
        return "Python dependencies"
    elif "npm" in step or "yarn" in step or "pnpm" in step:
        return "Node.js dependencies"
    return step


# =============================================================================
# Main Command
# =============================================================================

def env_command(options: EnvOptions) -> Tuple[int, str]:
    """
    Detect environment manifests and generate documentation.

    Returns:
        Tuple of (exit_code, output_message)
    """
    workspace = options.path

    # Detect manifests (v1.1: now returns tuple with warnings)
    manifests, parse_warnings = detect_manifests(workspace)

    # Generate onboarding steps
    onboarding = generate_onboarding(manifests)

    # Build result
    result = EnvResult(
        manifests=manifests,
        onboarding_steps=onboarding,
        parse_warnings=parse_warnings,  # v1.1: Include warnings
        generated_at=datetime.now(timezone.utc).isoformat(timespec="seconds"),
    )

    # Check for potential missing manifests
    has_node = any(m.details.get("runtimes", {}).get("node") for m in manifests)
    has_package_json = any(m.path.name == "package.json" for m in manifests)
    if has_node and not has_package_json:
        result.not_detected.append({
            "type": "npm_deps",
            "expected_file": "package.json",
            "reason": "node in .tool-versions but no package.json found",
        })

    # Handle --write flag
    if options.write:
        ontos_dir = workspace / ".ontos"
        ontos_dir.mkdir(exist_ok=True)
        env_md = ontos_dir / "environment.md"

        # v1.1: Safety check - require --force to overwrite existing file
        if env_md.exists() and not options.force:
            return 1, f"Error: {env_md} already exists. Use --force to overwrite."

        env_md.write_text(generate_environment_md(result, workspace))
        if not options.quiet:
            return 0, f"Environment documentation written to {env_md}"
        return 0, ""

    # Format output
    if options.format == "json":
        import json as json_module
        return 0, json_module.dumps(format_json_output(result), indent=2)

    return 0, format_text_output(result)
```

### 1.2 CLI Registration: `ontos/cli.py`

**Add registration call** (after line 99, with other registrations):
```python
_register_env(subparsers, global_parser)
```

**Add registration function** (near other registration functions, ~line 180):
```python
def _register_env(subparsers, parent):
    """Register env command."""
    p = subparsers.add_parser(
        "env",
        help="Detect and document environment manifests",
        parents=[parent]
    )
    p.add_argument(
        "--write", "-w",
        action="store_true",
        help="Write environment documentation to .ontos/environment.md"
    )
    p.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing environment.md (required with --write if file exists)"
    )
    p.add_argument(
        "--format", "-f",
        choices=["text", "json"],
        default="text",
        help="Output format (default: text)"
    )
    p.set_defaults(func=_cmd_env)
```

**Add handler function** (in handlers section, ~line 550):
```python
def _cmd_env(args) -> int:
    """Handle env command."""
    from ontos.commands.env import env_command, EnvOptions

    options = EnvOptions(
        path=Path.cwd(),
        write=getattr(args, "write", False),
        force=getattr(args, "force", False),  # v1.1: --force flag
        format=getattr(args, "format", "text"),
        quiet=args.quiet,
    )

    exit_code, output = env_command(options)

    if args.json or options.format == "json":
        # Already JSON formatted
        if output:
            print(output)
    elif not args.quiet and output:
        print(output)

    return exit_code
```

### 1.3 Doctor Integration: `ontos/commands/doctor.py`

**Add new check function** (after existing checks, ~line 454):
```python
def check_environment_manifests() -> CheckResult:
    """Check for presence of environment manifests."""
    from ontos.commands.env import detect_manifests

    try:
        workspace = Path.cwd()
        manifests = detect_manifests(workspace)

        if not manifests:
            return CheckResult(
                name="environment_manifests",
                status="warn",
                message="No environment manifests detected",
                details="Consider adding pyproject.toml, Brewfile, or package.json"
            )

        # Check for missing lock files
        missing_locks = [m for m in manifests if not m.lock_present and m.manifest_type in ("python_deps", "npm_deps")]

        if missing_locks:
            names = ", ".join(m.path.name for m in missing_locks)
            return CheckResult(
                name="environment_manifests",
                status="warn",
                message=f"Lock files missing for: {names}",
                details="Lock files ensure reproducible environments"
            )

        manifest_names = ", ".join(m.path.name for m in manifests)
        return CheckResult(
            name="environment_manifests",
            status="pass",
            message=f"Environment manifests present: {manifest_names}"
        )

    except Exception as e:
        return CheckResult(
            name="environment_manifests",
            status="warn",
            message="Could not check environment manifests",
            details=str(e)
        )
```

**Add to checks list** in `doctor_command()` (line ~509):
```python
checks = [
    check_configuration,
    check_git_hooks,
    check_python_version,
    check_docs_directory,
    check_context_map,
    check_validation,
    check_cli_availability,
    check_agents_staleness,
    check_environment_manifests,  # NEW
]
```

---

## Feature 2: Candidate Suggestions for Broken References

### 2.1 New Function: `ontos/core/suggestions.py`

**Add function** (after existing functions, ~line 130):
```python
from difflib import SequenceMatcher
from typing import List, Tuple, Dict
from ontos.core.types import DocumentData


def suggest_candidates_for_broken_ref(
    broken_ref: str,
    all_docs: Dict[str, DocumentData],
    referencing_doc: DocumentData = None,
    threshold: float = 0.5
) -> List[Tuple[str, float, str]]:
    """
    Generate candidate suggestions for a broken reference.

    Uses three matching strategies:
    1. Substring match (broken_ref appears in doc_id) -> 0.85 confidence
    2. Alias match (broken_ref matches doc aliases) -> 0.85 confidence
    3. Levenshtein distance (fuzzy string similarity) -> actual ratio as confidence

    Args:
        broken_ref: The invalid document ID reference
        all_docs: Dictionary mapping doc_id to DocumentData
        referencing_doc: The DocumentData object containing the broken ref (optional)
        threshold: Minimum similarity ratio for fuzzy matching (0.0-1.0)
                   Note: 0.5 is baseline for fuzzy; 0.85 is fixed for substring/alias

    Returns:
        List of (doc_id, confidence_score, reason) tuples,
        sorted by confidence descending (then alphabetically for ties), max 3 results

    Example:
        >>> suggest_candidates_for_broken_ref("finance_arch", docs)
        [('finance_engine_architecture', 0.87, 'substring match'), ...]
    """
    candidates = []
    broken_lower = broken_ref.lower()

    for doc_id, doc in all_docs.items():
        doc_id_lower = doc_id.lower()

        # Strategy 1: Substring match (high confidence: 0.85)
        # broken_ref is contained in doc_id OR doc_id is contained in broken_ref
        if broken_lower in doc_id_lower or doc_id_lower in broken_lower:
            candidates.append((doc_id, 0.85, "substring match"))
            continue

        # Strategy 2: Alias match (high confidence: 0.85)
        aliases = getattr(doc, 'aliases', []) or []
        if any(broken_lower in alias.lower() for alias in aliases):
            candidates.append((doc_id, 0.85, "alias match"))
            continue

        # Strategy 3: Levenshtein distance via SequenceMatcher (variable confidence)
        ratio = SequenceMatcher(None, broken_lower, doc_id_lower).ratio()
        if ratio >= threshold:
            candidates.append((doc_id, ratio, f"similarity: {ratio:.0%}"))

    # v1.1: Sort by confidence descending, then alphabetically by doc_id for deterministic ties
    return sorted(candidates, key=lambda x: (-x[1], x[0]))[:3]
```

### 2.2 Graph Integration: `ontos/core/graph.py`

**Add import** at top of file:
```python
from ontos.core.suggestions import suggest_candidates_for_broken_ref
```

**Modify broken link detection** (lines 62-72):

**Current code:**
```python
# Check for broken links
for dep_id in depends_on:
    if dep_id not in existing_ids:
        errors.append(ValidationError(
            error_type=ValidationErrorType.BROKEN_LINK,
            doc_id=doc_id,
            filepath=str(doc.filepath),
            message=f"Broken dependency: '{dep_id}' does not exist",
            fix_suggestion=f"Remove '{dep_id}' from depends_on or create the missing document",
            severity="error"
        ))
```

**New code:**
```python
# Check for broken links
for dep_id in depends_on:
    if dep_id not in existing_ids:
        # Generate candidate suggestions
        candidates = suggest_candidates_for_broken_ref(
            broken_ref=dep_id,
            all_docs=docs,
            referencing_doc=doc,
            threshold=0.5
        )

        # Build enriched suggestion
        if candidates:
            top = candidates[0]
            suggestion = f"Did you mean: '{top[0]}' ({top[1]:.0%} {top[2]})?"
            if len(candidates) > 1:
                second = candidates[1]
                suggestion += f" Or: '{second[0]}' ({second[1]:.0%})?"
        else:
            suggestion = f"Remove '{dep_id}' from depends_on or create the missing document"

        errors.append(ValidationError(
            error_type=ValidationErrorType.BROKEN_LINK,
            doc_id=doc_id,
            filepath=str(doc.filepath),
            message=f"Broken dependency: '{dep_id}' does not exist",
            fix_suggestion=suggestion,
            severity="error"
        ))
```

### 2.3 Map Output Enhancement: `ontos/commands/map.py`

**Modify `_generate_validation_section()`** (lines 175-191) to include `fix_suggestion`:

**Current code:**
```python
if result.errors:
    lines.append("")
    lines.append("### Errors")
    for error in result.errors:
        lines.append(f"- {error.doc_id}: {error.message}")
```

**New code:**
```python
if result.errors:
    lines.append("")
    lines.append("### Errors")
    for error in result.errors:
        lines.append(f"- **{error.doc_id}**: {error.message}")
        if error.fix_suggestion:
            lines.append(f"  - Suggestion: {error.fix_suggestion}")
```

---

## Test Plan

### 3.1 Environment Detection Tests

**New file: `tests/commands/test_env.py`**

```python
"""Tests for ontos env command."""

import pytest
from pathlib import Path
from ontos.commands.env import (
    detect_manifests,
    generate_onboarding,
    env_command,
    EnvOptions,
)


class TestManifestDetection:
    """Tests for manifest detection."""

    def test_detect_pyproject_toml(self, tmp_path):
        """Should detect pyproject.toml with poetry."""
        pyproject = tmp_path / "pyproject.toml"
        pyproject.write_text("""
[tool.poetry]
name = "test"
version = "0.1.0"

[tool.poetry.dependencies]
python = "^3.11"
click = "^8.0"
""")
        manifests = detect_manifests(tmp_path)
        assert len(manifests) == 1
        assert manifests[0].path.name == "pyproject.toml"
        assert manifests[0].package_manager == "poetry"

    def test_detect_brewfile(self, tmp_path):
        """Should detect Brewfile and parse formulae."""
        brewfile = tmp_path / "Brewfile"
        brewfile.write_text("""
brew "gh"
brew "jq"
cask "visual-studio-code"
""")
        manifests = detect_manifests(tmp_path)
        assert len(manifests) == 1
        assert manifests[0].details["formulae"] == ["gh", "jq"]
        assert manifests[0].details["casks"] == ["visual-studio-code"]

    def test_detect_tool_versions(self, tmp_path):
        """Should detect .tool-versions and parse runtimes."""
        tv = tmp_path / ".tool-versions"
        tv.write_text("python 3.11.4\nnode 20.10.0\n")
        manifests = detect_manifests(tmp_path)
        assert manifests[0].details["runtimes"] == {
            "python": "3.11.4",
            "node": "20.10.0",
        }

    def test_detect_lock_files(self, tmp_path):
        """Should detect associated lock files."""
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
        (tmp_path / "poetry.lock").write_text("# lock")
        manifests = detect_manifests(tmp_path)
        assert manifests[0].lock_present is True
        assert manifests[0].lock_file.name == "poetry.lock"

    def test_no_manifests(self, tmp_path):
        """Should return empty list when no manifests found."""
        manifests = detect_manifests(tmp_path)
        assert manifests == []


class TestOnboardingGeneration:
    """Tests for onboarding step generation."""

    def test_correct_order(self, tmp_path):
        """Should generate steps in correct dependency order."""
        (tmp_path / "Brewfile").write_text("brew 'gh'")
        (tmp_path / ".tool-versions").write_text("python 3.11")
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")

        manifests = detect_manifests(tmp_path)
        steps = generate_onboarding(manifests)

        # System deps first, then runtimes, then Python
        assert steps[0] == "brew bundle"
        assert steps[1] == "asdf install"
        assert steps[2] == "poetry install"


class TestEnvCommand:
    """Tests for env_command."""

    def test_text_output(self, tmp_path, monkeypatch):
        """Should produce readable text output."""
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path)
        code, output = env_command(options)

        assert code == 0
        assert "pyproject.toml" in output
        assert "Detected Manifests" in output

    def test_json_output(self, tmp_path, monkeypatch):
        """Should produce valid JSON output."""
        import json
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path, format="json")
        code, output = env_command(options)

        assert code == 0
        data = json.loads(output)
        assert data["$schema"] == "ontos-env-v1"
        assert len(data["manifests"]) == 1

    def test_write_flag(self, tmp_path, monkeypatch):
        """Should write .ontos/environment.md."""
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path, write=True, force=True)
        code, output = env_command(options)

        assert code == 0
        env_md = tmp_path / ".ontos" / "environment.md"
        assert env_md.exists()
        content = env_md.read_text()
        assert "Auto-generated by `ontos env`" in content

    # v1.1: New tests for --force flag
    def test_write_requires_force_when_file_exists(self, tmp_path, monkeypatch):
        """Should error when environment.md exists without --force."""
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
        ontos_dir = tmp_path / ".ontos"
        ontos_dir.mkdir()
        (ontos_dir / "environment.md").write_text("# Manual content")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path, write=True, force=False)
        code, output = env_command(options)

        assert code == 1
        assert "already exists" in output
        assert "--force" in output

    def test_write_with_force_overwrites(self, tmp_path, monkeypatch):
        """Should overwrite when --force is provided."""
        (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
        ontos_dir = tmp_path / ".ontos"
        ontos_dir.mkdir()
        (ontos_dir / "environment.md").write_text("# Manual content")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path, write=True, force=True)
        code, output = env_command(options)

        assert code == 0
        content = (ontos_dir / "environment.md").read_text()
        assert "Auto-generated" in content

    # v1.1: New tests for parse warnings
    def test_malformed_manifest_shows_warning(self, tmp_path, monkeypatch):
        """Should show warning when manifest parsing fails."""
        (tmp_path / "pyproject.toml").write_text("invalid [ toml {")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path)
        code, output = env_command(options)

        assert code == 0  # Detection still succeeds
        assert "pyproject.toml" in output  # File detected
        assert "Parse Warnings" in output or "parse failed" in output

    def test_parse_warnings_in_json_output(self, tmp_path, monkeypatch):
        """Should include parse_warnings in JSON output."""
        import json
        (tmp_path / "pyproject.toml").write_text("invalid [ toml {")
        monkeypatch.chdir(tmp_path)

        options = EnvOptions(path=tmp_path, format="json")
        code, output = env_command(options)

        assert code == 0
        data = json.loads(output)
        assert "parse_warnings" in data
        assert len(data["parse_warnings"]) > 0
```

### 3.2 Candidate Suggestions Tests

**New tests in: `tests/test_suggestions.py`**

```python
"""Tests for candidate suggestion function."""

import pytest
from ontos.core.suggestions import suggest_candidates_for_broken_ref
from ontos.core.types import DocumentData
from pathlib import Path


@pytest.fixture
def sample_docs():
    """Create sample document dictionary."""
    def make_doc(doc_id, aliases=None):
        return DocumentData(
            doc_id=doc_id,
            filepath=Path(f"{doc_id}.md"),
            type="atom",
            status="active",
            aliases=aliases or [],
            depends_on=[],
            tags=[],
        )

    return {
        "finance_engine_architecture": make_doc(
            "finance_engine_architecture",
            aliases=["finance_arch", "fin_arch"]
        ),
        "finance_api_spec": make_doc("finance_api_spec"),
        "user_guide": make_doc("user_guide"),
        "api_reference": make_doc("api_reference"),
    }


class TestCandidateSuggestions:
    """Tests for suggest_candidates_for_broken_ref."""

    def test_substring_match(self, sample_docs):
        """Should find candidates via substring match."""
        candidates = suggest_candidates_for_broken_ref(
            "finance_arch",
            sample_docs
        )
        assert len(candidates) >= 1
        assert candidates[0][0] == "finance_engine_architecture"
        assert candidates[0][1] >= 0.8
        assert "substring" in candidates[0][2]

    def test_alias_match(self, sample_docs):
        """Should find candidates via alias match."""
        candidates = suggest_candidates_for_broken_ref(
            "fin_arch",
            sample_docs
        )
        assert len(candidates) >= 1
        assert candidates[0][0] == "finance_engine_architecture"
        assert "alias" in candidates[0][2]

    def test_levenshtein_match(self, sample_docs):
        """Should find candidates via similarity."""
        candidates = suggest_candidates_for_broken_ref(
            "user_guid",  # Typo
            sample_docs
        )
        assert len(candidates) >= 1
        assert candidates[0][0] == "user_guide"
        assert "similarity" in candidates[0][2]

    def test_no_matches(self, sample_docs):
        """Should return empty list when no matches."""
        candidates = suggest_candidates_for_broken_ref(
            "completely_unrelated_id",
            sample_docs,
            threshold=0.8
        )
        assert candidates == []

    def test_max_three_results(self, sample_docs):
        """Should return at most 3 candidates."""
        # Add more similar docs
        for i in range(10):
            sample_docs[f"finance_doc_{i}"] = sample_docs["finance_api_spec"]

        candidates = suggest_candidates_for_broken_ref(
            "finance",
            sample_docs
        )
        assert len(candidates) <= 3

    def test_sorted_by_confidence(self, sample_docs):
        """Should return candidates sorted by confidence descending."""
        candidates = suggest_candidates_for_broken_ref(
            "finance",
            sample_docs
        )
        if len(candidates) >= 2:
            assert candidates[0][1] >= candidates[1][1]
```

### 3.3 Integration Tests

**Add to: `tests/integration/test_cli.py`**

```python
def test_env_command_integration(tmp_path, monkeypatch):
    """Test ontos env command end-to-end."""
    # Setup project
    (tmp_path / "pyproject.toml").write_text("[tool.poetry]\nname='test'")
    monkeypatch.chdir(tmp_path)

    # Run command
    result = subprocess.run(
        ["python", "-m", "ontos", "env"],
        capture_output=True,
        text=True
    )

    assert result.returncode == 0
    assert "pyproject.toml" in result.stdout


def test_map_shows_candidate_suggestions(tmp_path, monkeypatch):
    """Test that map command shows suggestions for broken refs."""
    # Setup project with broken reference
    docs_dir = tmp_path / "docs"
    docs_dir.mkdir()

    (docs_dir / "spec.md").write_text("""---
id: spec
type: atom
depends_on: [arch]
---
# Spec
""")

    (docs_dir / "architecture.md").write_text("""---
id: architecture
type: atom
---
# Architecture
""")

    # Run ontos map
    monkeypatch.chdir(tmp_path)
    result = subprocess.run(
        ["python", "-m", "ontos", "map"],
        capture_output=True,
        text=True
    )

    # Should show suggestion
    assert "Did you mean" in result.stdout or "architecture" in result.stdout
```

---

## File Change Summary

| File | Change Type | Lines Changed (Est.) | v1.1 Notes |
|------|-------------|---------------------|------------|
| `ontos/commands/env.py` | **NEW** | ~380 | +parse warnings, +force flag |
| `ontos/cli.py` | MODIFY | +35 | +force arg |
| `ontos/commands/doctor.py` | MODIFY | +40 | — |
| `ontos/core/suggestions.py` | MODIFY | +55 | +deterministic sort |
| `ontos/core/graph.py` | MODIFY | +15 | — |
| `ontos/commands/map.py` | MODIFY | +5 | — |
| `tests/commands/test_env.py` | **NEW** | ~150 | +force tests, +warning tests |
| `tests/test_suggestions.py` | MODIFY | +60 | — |
| `docs/reference/Ontos_Manual.md` | MODIFY | +30 | — |

**Total:** ~770 new/modified lines (v1.1: +90 from v1.0)

---

## Verification Plan

### Manual Verification

```bash
# 1. Test env command basic functionality
cd /path/to/project/with/pyproject.toml
ontos env
# Expected: Shows detected manifests, onboarding steps

# 2. Test env --write
ontos env --write
cat .ontos/environment.md
# Expected: Generated markdown with manifest details

# 3. Test env --format json
ontos env --format json | jq .
# Expected: Valid JSON with schema "ontos-env-v1"

# 4. Test doctor includes env check
ontos doctor
# Expected: Shows "environment_manifests" check result

# 5. Test candidate suggestions
# Create doc with broken ref, run ontos map
echo '---
id: test
depends_on: [nonexistent_arch]
---
# Test' > docs/test.md
ontos map
# Expected: Error shows "Did you mean: architecture (85%)?"

# 6. Verify no regressions
pytest tests/ -v
```

### Automated CI Verification

```yaml
# Add to CI workflow
- name: Test env command
  run: |
    ontos env --format json | jq -e '.manifests'
    ontos env --write
    test -f .ontos/environment.md
```

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| `tomllib` requires Python 3.11+ | Users on 3.9/3.10 cannot parse pyproject.toml | v1.1: Fallback import `tomli as tomllib` for 3.9/3.10 |
| Manifest parsing fails | Degraded output | v1.1: Parse warnings surfaced to user via `parse_warnings` list |
| Too many suggestions slow down map | Performance | Cap candidates at 3, use set for existing_ids |
| Platform-specific manifests (Brewfile on Linux) | Confusing output | Label with "(macOS)" in output |
| Stale environment.md | Documentation drift | Include timestamp + regeneration command |
| `--write` overwrites manual edits | Lost documentation | v1.1: Require `--force` flag to overwrite existing file |
| Symlinked manifests outside workspace | Potential path traversal (read-only) | Known limitation; low risk as operation is read-only |

---

## Success Criteria

| Metric | Target |
|--------|--------|
| `ontos env` detects all Phase 1 manifest types | 100% |
| Candidate suggestions found for typo-based broken refs | > 80% |
| Test coverage for new code | > 90% |
| No new dependencies added | 0 |
| No regressions in existing tests | 100% pass |
| Doctor run time impact | < 100ms additional |

---

## Review Checklist

- [ ] Does Environment Detection align with "Document, don't manage" philosophy?
- [ ] Is the scope appropriate for v3.2.0?
- [ ] Are all edge cases handled gracefully?
- [ ] Is the implementation plan detailed enough to execute?
- [ ] Are deferred items clearly documented?
- [ ] Is the test plan comprehensive?
- [ ] Are risks properly mitigated?
