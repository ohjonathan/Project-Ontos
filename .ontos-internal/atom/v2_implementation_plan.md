---
id: v2_implementation_plan
type: atom
status: draft
depends_on: [v2_architecture]
---

# Ontos 2.0 Implementation Plan

**Document Type:** Implementation Specification  
**Status:** Approved for Implementation  
**Version:** 1.1 (Consolidated)  
**Reviewers:** Gemini CLI âœ…, Claude âœ…  
**Author:** Claude (Anthropic) + Johnny  
**Date:** 2025-12-12  

---

## Executive Summary

This document provides a comprehensive, step-by-step implementation plan for transforming Project Ontos from v1.5 (current state) to v2.0 (target state). The plan is organized into three phases matching the v2 strategy roadmap, with each phase containing discrete, testable deliverables.

**The Core Change:** Ontos v2.0 introduces the "Dual Ontology" modelâ€”separating **Space** (what IS true about the project) from **Time** (what HAPPENED during development). This is implemented primarily through elevating `log` from a pseudo-type to a first-class citizen with its own schema, validation rules, and dedicated context map section.

**Source of Truth Documents:**
- `.ontos-internal/strategy/v2_strategy.md` â€” Strategic vision and phases
- `.ontos-internal/atom/v2_technical_architecture.md` â€” Technical specifications

**Review History:**
- v1.0 (2025-12-12): Initial plan generated by Claude
- v1.1 (2025-12-12): Consolidated with Gemini-reviewed addendum (R1-R5)

---

## Table of Contents

1. [Current State Analysis](#1-current-state-analysis)
2. [Target State Overview](#2-target-state-overview)
3. [Phase 1: Structure (v2.0.x)](#3-phase-1-structure-v20x)
4. [Phase 2: Visibility (v2.1.x)](#4-phase-2-visibility-v21x)
5. [Phase 3: Intelligence (v2.2.x)](#5-phase-3-intelligence-v22x)
6. [Testing Strategy](#6-testing-strategy)
7. [Migration & Backward Compatibility](#7-migration--backward-compatibility)
8. [Risk Assessment](#8-risk-assessment)
9. [Implementation Checklist](#9-implementation-checklist)

---

## 1. Current State Analysis

### 1.1 What Ontos 1.5 Already Has

Before implementing v2.0, we must understand what already exists. The current codebase (v1.5.0) includes:

| Component | Status | Location |
|-----------|--------|----------|
| `log` type in TYPE_DEFINITIONS | âš ï¸ Implicit | `ontos_config_defaults.py` |
| Smart config (Contributor/User mode) | âœ… Exists | `ontos_config.py` |
| Session archival script | âœ… Basic | `ontos_end_session.py` |
| Context map with LOG section | âœ… Basic | `ontos_generate_context_map.py` |
| Log files created with `type: atom` | âš ï¸ Legacy | `docs/logs/*.md` |

**Critical Observations:**
1. The `ontos_end_session.py` script generates logs with `type: atom` (not `type: log`)
2. No validation exists for log-specific fields (`event_type`, `impacts`)
3. The context map doesn't have a Timeline section
4. No auto-suggest impacts functionality exists
5. The `log` type is not explicitly defined in TYPE_DEFINITIONS

### 1.2 File Inventory

**Scripts that need modification:**

| File | Changes Required |
|------|------------------|
| `ontos_config_defaults.py` | Add explicit `log` type with `allows_depends_on`, add `EVENT_TYPES` constant |
| `ontos_generate_context_map.py` | Add Timeline section, token estimates, provenance header, log validation with archived leniency |
| `ontos_end_session.py` | Add `--event-type`, `--concepts`, `--impacts` flags, enhanced auto-suggest with git log fallback |
| `ontos_update.py` | Add root-level file handling for `ontos_init.py` |
| `ontos_config.py` | No changes (user config preserved) |

**Scripts to create:**

| File | Purpose |
|------|---------|
| `ontos_init.py` | Unified initialization (combines install + initiate) |
| `ontos_migrate_v2.py` | Optional explicit v1â†’v2 log migration |
| `ontos_summarize.py` | Generate 50-word summaries (Phase 3) |

### 1.3 Current Log File Format (v1.x)

```yaml
---
id: log_20251212_session_name
type: atom  # â† This is the problem: logs are typed as atoms
status: active
depends_on: []
---
```

### 1.4 Target Log File Format (v2.0)

```yaml
---
id: log_20251212_session_name
type: log  # â† First-class type
status: active
event_type: feature  # â† NEW: Required field
concepts: [auth, oauth]  # â† NEW: Optional searchability tags
impacts: [auth_flow, api_spec]  # â† NEW: Required field linking to Space
---
```

---

## 2. Target State Overview

### 2.1 The Dual Ontology Model

v2.0 introduces a fundamental conceptual shift:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SPACE (Truth)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  kernel  â”‚â”€â”€â”€â–¶â”‚ strategy â”‚â”€â”€â”€â–¶â”‚ product  â”‚â”€â”€â”€â–¶â”‚   atom   â”‚  â”‚
â”‚  â”‚  rank 0  â”‚    â”‚  rank 1  â”‚    â”‚  rank 2  â”‚    â”‚  rank 3  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â–²                                  â”‚
â”‚                              â”‚ impacts: []                      â”‚
â”‚                              â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      TIME (History)                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚ log â”‚  â”‚ log â”‚  â”‚ log â”‚  â”‚ log â”‚  â”‚ log â”‚  â”‚ log â”‚    â”‚ â”‚
â”‚  â”‚  â”‚rank4â”‚  â”‚rank4â”‚  â”‚rank4â”‚  â”‚rank4â”‚  â”‚rank4â”‚  â”‚rank4â”‚    â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â”‚ â”‚
â”‚  â”‚                        time                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Architectural Principles:**

1. **Space documents use `depends_on`** â€” They form a directed acyclic graph of truth
2. **Time documents use `impacts`** â€” They point backward to Space, recording what they affected
3. **History is immutable** â€” A log records what was true *at that moment*; deleting a Space document doesn't invalidate historical logs that referenced it

### 2.2 Phased Delivery

| Phase | Version | Focus | Deliverables |
|-------|---------|-------|--------------|
| 1: Structure | v2.0.x | Schema & validation | Unified init, log validation, impacts validation, enhanced archival |
| 2: Visibility | v2.1.x | Context map enhancements | Token estimates, Timeline section, provenance header |
| 3: Intelligence | v2.2.x | AI-powered features | Summary generation, auto-activation |

---

## 3. Phase 1: Structure (v2.0.x)

Phase 1 establishes the foundational schema changes and validation rules. This is the most critical phaseâ€”it defines the data model that all future features depend on.

### 3.1 Deliverable 1: Type Definitions & Event Taxonomy

**What:** Explicitly define `log` as a first-class type and create the event type taxonomy.

**Why:** The `log` type must be formally defined in TYPE_DEFINITIONS so validation scripts recognize it. The `allows_depends_on` field encodes the Space/Time distinction in configuration rather than scattered `if type == 'log'` checks.

**Implementation:**

**File:** `.ontos/scripts/ontos_config_defaults.py`

```python
# =============================================================================
# TYPE DEFINITIONS (v2.0)
# =============================================================================
# The Dual Ontology: Space (Truth) vs Time (History)
#
# Space types form a directed acyclic graph via depends_on
# Time types (logs) connect to Space via impacts, not depends_on

TYPE_DEFINITIONS = {
    # --- SPACE ONTOLOGY (Truth) ---
    # These document what IS true about the project
    'kernel': {
        'rank': 0,
        'description': 'Core identity documents (mission, principles)',
        'allows_depends_on': True,
    },
    'strategy': {
        'rank': 1,
        'description': 'High-level direction and decisions',
        'allows_depends_on': True,
    },
    'product': {
        'rank': 2,
        'description': 'Feature specifications and requirements',
        'allows_depends_on': True,
    },
    'atom': {
        'rank': 3,
        'description': 'Implementation details and technical specs',
        'allows_depends_on': True,
    },
    # --- TIME ONTOLOGY (History) ---
    # These document what HAPPENED during development
    'log': {
        'rank': 4,
        'description': 'Session history (what happened, not what is)',
        'allows_depends_on': False,  # Logs use 'impacts' instead
    },
}

# Valid types for early validation
VALID_TYPES = set(TYPE_DEFINITIONS.keys())


# =============================================================================
# EVENT TYPES (v2.0+)
# =============================================================================
# Captures the PRIMARY intent of a development session
# Used in log documents to categorize work

EVENT_TYPES = {
    'feature': {
        'definition': 'Adding new capability to the system',
        'examples': ['Implemented OAuth login', 'Added search functionality'],
    },
    'fix': {
        'definition': 'Correcting broken or incorrect behavior',
        'examples': ['Fixed refresh token bug', 'Resolved race condition'],
    },
    'refactor': {
        'definition': 'Restructuring code without changing behavior',
        'examples': ['Split auth module', 'Migrated to TypeScript'],
    },
    'exploration': {
        'definition': 'Research, spikes, or prototypes',
        'examples': ['Evaluated database options', 'Tested new library'],
    },
    'chore': {
        'definition': 'Maintenance, dependencies, configuration',
        'examples': ['Updated dependencies', 'Fixed CI pipeline'],
    },
}

# Valid event type values (for validation)
VALID_EVENT_TYPES = set(EVENT_TYPES.keys())
```

**Rationale:**
- The `allows_depends_on` field makes the Space/Time distinction explicit in data, not code
- The taxonomy is deliberately small (5 types)â€”more granular categorization uses the `concepts` field
- `VALID_TYPES` enables early validation before schema-specific checks

---

### 3.2 Deliverable 2: Log Type Schema Validation

**What:** Add validation rules that enforce the v2.0 log schema.

**Why:** Without validation, agents will create logs missing required fields, breaking the Dual Ontology model. Validation catches errors at creation time, not when someone tries to use the Timeline.

**Implementation:**

**File:** `.ontos/scripts/ontos_generate_context_map.py`

**Add new validation function after `validate_dependencies()`:**

```python
def validate_log_schema(files_data: dict[str, dict]) -> list[str]:
    """Validate log-type documents have required v2.0 fields.
    
    Rules:
    1. Logs MUST have event_type field
    2. event_type MUST be one of: feature, fix, refactor, exploration, chore
    3. Logs MUST have impacts field (can be empty list)
    4. Logs SHOULD NOT have depends_on (warning, not error)
    
    Args:
        files_data: Dictionary of document metadata.
        
    Returns:
        List of issue strings.
    """
    from ontos_config_defaults import VALID_EVENT_TYPES, TYPE_DEFINITIONS
    
    issues = []
    
    for doc_id, data in files_data.items():
        if data['type'] != 'log':
            continue
            
        filepath = data['filepath']
        
        # Rule 1: event_type is required
        event_type = data.get('event_type')
        if event_type is None:
            issues.append(
                f"- [MISSING FIELD] **{doc_id}** ({filepath}) is type 'log' but missing required field: event_type\n"
                f"  Fix: Add `event_type: feature|fix|refactor|exploration|chore` to frontmatter"
            )
        # Rule 2: event_type must be valid
        elif event_type not in VALID_EVENT_TYPES:
            issues.append(
                f"- [INVALID VALUE] **{doc_id}** ({filepath}) has invalid event_type: '{event_type}'\n"
                f"  Fix: Use one of: {', '.join(sorted(VALID_EVENT_TYPES))}"
            )
        
        # Rule 3: impacts is required (empty list is OK)
        if 'impacts' not in data:
            issues.append(
                f"- [MISSING FIELD] **{doc_id}** ({filepath}) is type 'log' but missing required field: impacts\n"
                f"  Fix: Add `impacts: []` or `impacts: [doc_id1, doc_id2]` to frontmatter"
            )
        
        # Rule 4: depends_on should not be used (check config for allows_depends_on)
        type_config = TYPE_DEFINITIONS.get('log', {})
        if not type_config.get('allows_depends_on', True):
            if data.get('depends_on') and len(data['depends_on']) > 0:
                issues.append(
                    f"- [WARNING] **{doc_id}** ({filepath}) is type 'log' but has depends_on field\n"
                    f"  Fix: Logs should use `impacts` instead of `depends_on`. Remove depends_on."
                )
    
    return issues
```

**Modify `scan_docs()` to extract new fields:**

```python
def scan_docs(root_dirs: list[str]) -> dict[str, dict]:
    """Scans directories for markdown files and parses their metadata."""
    files_data = {}
    for root_dir in root_dirs:
        # ... existing code ...
        
        if frontmatter and 'id' in frontmatter:
            # ... existing normalization ...
            
            files_data[doc_id] = {
                'filepath': filepath,
                'filename': file,
                'type': normalize_type(frontmatter.get('type')),
                'depends_on': normalize_depends_on(frontmatter.get('depends_on')),
                'status': str(frontmatter.get('status') or 'unknown').strip() or 'unknown',
                # NEW v2.0 fields for logs
                'event_type': frontmatter.get('event_type'),
                'concepts': frontmatter.get('concepts', []),
                'impacts': frontmatter.get('impacts', []),
            }
    return files_data
```

**Update `generate_context_map()` to call new validation:**

```python
def generate_context_map(target_dirs: list[str], quiet: bool = False) -> int:
    # ... existing code ...
    
    if not quiet:
        print("Validating dependencies...")
    issues = validate_dependencies(files_data)
    
    # NEW: Validate log schema
    if not quiet:
        print("Validating log schema...")
    log_issues = validate_log_schema(files_data)
    issues.extend(log_issues)
    
    # ... rest of function ...
```

---

### 3.3 Deliverable 3: Impacts Validation (The Bridge)

**What:** Validate that all IDs in `impacts[]` refer to existing Space documents, with leniency for archived logs.

**Why:** The `impacts` field is the bridge between Time and Space. If it points to non-existent documents, the bridge is broken. However, **History is immutable**â€”archived logs shouldn't break just because a Space document was later deleted.

**Implementation:**

**File:** `.ontos/scripts/ontos_generate_context_map.py`

```python
def validate_impacts(files_data: dict[str, dict], strict: bool = False) -> list[str]:
    """Validate that impacts[] references exist in Space Ontology.
    
    The impacts field connects History (logs) to Truth (space documents).
    
    Validation Rules:
    - Active logs: impacts MUST reference existing Space documents (ERROR)
    - Archived logs: broken references are noted but not errors (INFO)
    
    Rationale: History is immutable. If a Space document is deleted,
    historical logs that referenced it shouldn't turn red. The log
    recorded what was true at that moment.
    
    Args:
        files_data: Dictionary of document metadata.
        strict: If True, treat archived log broken links as errors too.
        
    Returns:
        List of issue strings.
    """
    issues = []
    
    # Build set of Space document IDs (everything except logs)
    space_ids = {
        doc_id for doc_id, data in files_data.items()
        if data['type'] != 'log'
    }
    
    for doc_id, data in files_data.items():
        if data['type'] != 'log':
            continue
        
        # Check if this log is archived (historical)
        is_archived = data.get('status') == 'archived'
        
        impacts = data.get('impacts', [])
        if not isinstance(impacts, list):
            impacts = [impacts] if impacts else []
        
        for impact_id in impacts:
            # Check if impact references another log (not allowed)
            if impact_id.startswith('log_'):
                issues.append(
                    f"- [INVALID REFERENCE] **{doc_id}** ({data['filepath']}) "
                    f"impacts references another log: `{impact_id}`\n"
                    f"  Fix: impacts should reference Space documents, not other logs"
                )
                continue
            
            if impact_id not in space_ids:
                if is_archived and not strict:
                    # =========================================================
                    # ARCHIVED LOGS: Informational only
                    # History is immutableâ€”don't error on deleted references
                    # =========================================================
                    issues.append(
                        f"- [INFO] **{doc_id}** references deleted document `{impact_id}` "
                        f"(archived log, no action needed)"
                    )
                else:
                    # =========================================================
                    # ACTIVE LOGS: This is a real problem
                    # Either the reference is wrong or the doc needs to exist
                    # =========================================================
                    issues.append(
                        f"- [BROKEN LINK] **{doc_id}** ({data['filepath']}) "
                        f"impacts non-existent document: `{impact_id}`\n"
                        f"  Fix: Create `{impact_id}`, correct the reference, or archive this log"
                    )
    
    return issues
```

**Update `generate_context_map()` to call impacts validation:**

```python
def generate_context_map(target_dirs: list[str], quiet: bool = False, strict: bool = False) -> int:
    # ... existing code ...
    
    if not quiet:
        print("Validating log schema...")
    log_issues = validate_log_schema(files_data)
    issues.extend(log_issues)
    
    # NEW: Validate impacts references
    if not quiet:
        print("Validating impacts references...")
    impact_issues = validate_impacts(files_data, strict=strict)
    issues.extend(impact_issues)
    
    # ... rest of function ...
```

**Add `--strict` flag to argparse:**

```python
parser.add_argument('--strict', action='store_true',
                    help='Treat archived log broken links as errors (for CI)')
```

---

### 3.4 Deliverable 4: Enhanced Session Archival

**What:** Upgrade `ontos_end_session.py` to:
1. Generate `type: log` instead of `type: atom`
2. Accept `--event-type`, `--concepts`, `--impacts` flags
3. Auto-suggest impacts based on git changes AND today's commits

**Why:** This is the primary entry point for creating log documents. The enhanced git logic captures work even after it's been committed (the "commit then archive" workflow).

**Implementation:**

**File:** `.ontos/scripts/ontos_end_session.py`

**Update argparse to add new flags:**

```python
def main() -> None:
    parser = argparse.ArgumentParser(
        description='Scaffold a new session log file with v2.0 schema support.',
        # ... existing setup ...
    )
    
    # Existing arguments
    parser.add_argument('topic', type=str, nargs='?', 
                        help='Short slug describing the session (e.g. auth-refactor)')
    parser.add_argument('--quiet', '-q', action='store_true', 
                        help='Suppress non-error output')
    parser.add_argument('--source', '-s', type=str, metavar='NAME',
                        help='LLM/program that generated this log')
    
    # NEW v2.0 arguments
    parser.add_argument('--event-type', '-e', type=str, metavar='TYPE',
                        choices=['feature', 'fix', 'refactor', 'exploration', 'chore'],
                        help='Type of work performed (feature/fix/refactor/exploration/chore)')
    parser.add_argument('--concepts', type=str, metavar='TAGS',
                        help='Comma-separated concept tags (e.g., "auth,oauth,security")')
    parser.add_argument('--impacts', type=str, metavar='IDS',
                        help='Comma-separated doc IDs impacted (e.g., "auth_flow,api_spec")')
    parser.add_argument('--suggest-impacts', action='store_true',
                        help='Auto-suggest impacts based on git changes')
    
    # Existing changelog arguments...
```

**Add enhanced auto-suggest impacts algorithm:**

```python
def load_document_index() -> dict[str, str]:
    """Load mapping of filepaths to document IDs from context map.
    
    Returns:
        Dictionary mapping relative filepath to doc_id.
    """
    from ontos_config import CONTEXT_MAP_FILE
    
    if not os.path.exists(CONTEXT_MAP_FILE):
        return {}
    
    index = {}
    
    # Parse the Index section of the context map
    with open(CONTEXT_MAP_FILE, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Find the Index table
    in_index = False
    for line in content.split('\n'):
        if '## ' in line and 'Index' in line:
            in_index = True
            continue
        if in_index and line.startswith('|') and '|' in line[1:]:
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 4 and parts[1] and parts[2]:
                doc_id = parts[1]
                # Extract filepath from markdown link if present
                filename_part = parts[2]
                if '(' in filename_part and ')' in filename_part:
                    filepath = filename_part.split('(')[1].split(')')[0]
                else:
                    filepath = filename_part
                if doc_id and doc_id != 'ID' and filepath:
                    index[filepath] = doc_id
        if in_index and line.startswith('#') and not line.startswith('|'):
            break
    
    return index


def suggest_impacts(quiet: bool = False) -> list[str]:
    """Suggest document IDs that may have been impacted by recent changes.
    
    Algorithm:
    1. Check uncommitted changes (git status)
    2. If clean, check commits made today (git log --since)
    3. Match changed files to known document paths
    4. Return matching IDs as suggestions
    
    This handles both the "work in progress" and "commit then archive" workflows.
    
    Returns:
        List of suggested document IDs.
    """
    import datetime
    
    try:
        changed_files = set()
        
        # =====================================================
        # STEP 1: Check uncommitted changes (staged + unstaged)
        # =====================================================
        result = subprocess.run(
            ['git', 'status', '--porcelain'],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            for line in result.stdout.strip().split('\n'):
                if line:
                    # Format: "XY filename" or "XY original -> renamed"
                    parts = line[3:].split(' -> ')
                    changed_files.add(parts[-1])
        
        # =====================================================
        # STEP 2: If nothing uncommitted, check today's commits
        # This handles the "commit then archive" workflow
        # =====================================================
        if not changed_files:
            today = datetime.date.today().isoformat()
            result = subprocess.run(
                ['git', 'log', '--since', today, '--name-only', '--pretty=format:'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line:
                        changed_files.add(line)
            
            if not quiet and changed_files:
                print(f"â„¹ï¸  No uncommitted changes; using today's commits instead")
        
        if not changed_files:
            return []
        
        # =====================================================
        # STEP 3: Match changed files to document IDs
        # =====================================================
        doc_index = load_document_index()
        if not doc_index:
            return []
        
        suggestions = set()
        for changed_file in changed_files:
            # Direct match
            if changed_file in doc_index:
                suggestions.add(doc_index[changed_file])
            
            # Directory-based match
            changed_dir = os.path.dirname(changed_file)
            if changed_dir:
                for doc_path, doc_id in doc_index.items():
                    doc_dir = os.path.dirname(doc_path)
                    if doc_dir and (doc_dir == changed_dir or changed_dir.startswith(doc_dir)):
                        suggestions.add(doc_id)
        
        # Filter out log documents (impacts should reference Space, not Time)
        final_suggestions = [s for s in suggestions if not s.startswith('log_')]
        
        if not quiet and final_suggestions:
            print(f"\nðŸ’¡ Suggested impacts based on changes: {', '.join(final_suggestions)}")
        
        return final_suggestions
        
    except Exception as e:
        if not quiet:
            print(f"Warning: Could not suggest impacts: {e}")
        return []


def prompt_for_impacts(suggestions: list[str], quiet: bool = False) -> list[str]:
    """Interactive prompt for impact confirmation.
    
    Args:
        suggestions: Pre-computed suggestions from git analysis.
        quiet: If True, skip prompts and return suggestions as-is.
        
    Returns:
        Final list of impact IDs.
    """
    if quiet:
        return suggestions
    
    if suggestions:
        print(f"\nSuggested impacts: {', '.join(suggestions)}")
        try:
            response = input("Accept suggestions? [Y/n/edit]: ").strip().lower()
            
            if response in ('', 'y', 'yes'):
                return suggestions
            elif response in ('n', 'no'):
                manual = input("Enter impacts (comma-separated, or empty): ").strip()
                return [i.strip() for i in manual.split(',') if i.strip()] if manual else []
            else:
                # Edit mode - start with suggestions
                manual = input(f"Edit impacts (starting with {', '.join(suggestions)}): ").strip()
                return [i.strip() for i in manual.split(',') if i.strip()] if manual else suggestions
        except (EOFError, KeyboardInterrupt):
            print("\nUsing suggestions.")
            return suggestions
    else:
        try:
            manual = input("Enter impacts (comma-separated doc IDs, or empty): ").strip()
            return [i.strip() for i in manual.split(',') if i.strip()] if manual else []
        except (EOFError, KeyboardInterrupt):
            return []
```

**Update `create_log_file()` to generate v2.0 schema:**

```python
def create_log_file(
    topic_slug: str, 
    quiet: bool = False, 
    source: str = "",
    event_type: str = "chore",
    concepts: list[str] = None,
    impacts: list[str] = None
) -> str:
    """Creates a new session log file with v2.0 schema.
    
    Args:
        topic_slug: Short slug describing the session.
        quiet: Suppress output if True.
        source: LLM/program that generated this log.
        event_type: Type of work (feature/fix/refactor/exploration/chore).
        concepts: List of concept tags for searchability.
        impacts: List of document IDs affected by this session.
        
    Returns:
        Path to the created log file, or empty string on error.
    """
    # Normalize inputs
    topic_slug = topic_slug.lower()
    concepts = concepts or []
    impacts = impacts or []
    
    # ... existing directory creation ...
    
    now = datetime.datetime.now().astimezone()
    today_date = now.strftime("%Y-%m-%d")
    today_datetime = now.strftime("%Y-%m-%d %H:%M %Z")
    filename = f"{today_date}_{topic_slug}.md"
    filepath = os.path.join(LOGS_DIR, filename)
    
    # ... existing file check ...
    
    daily_log = get_session_git_log()
    source_line = f"\nSource: {source}" if source else ""
    
    # Format concepts and impacts for YAML
    concepts_yaml = f"[{', '.join(concepts)}]" if concepts else "[]"
    impacts_yaml = f"[{', '.join(impacts)}]" if impacts else "[]"
    
    # v2.0 SCHEMA
    content = f"""---
id: log_{today_date.replace('-', '')}_{topic_slug.replace('-', '_')}
type: log
status: active
event_type: {event_type}
concepts: {concepts_yaml}
impacts: {impacts_yaml}
---

# Session Log: {topic_slug.replace('-', ' ').title()}
Date: {today_datetime}{source_line}
Event Type: {event_type}

## 1. Goal
<!-- [AGENT: Fill this in. What was the primary objective of this session?] -->

## 2. Key Decisions
<!-- [AGENT: Fill this in. What architectural or design choices were made?] -->
-

## 3. Changes Made
<!-- [AGENT: Fill this in. Summary of file changes.] -->
-

## 4. Next Steps
<!-- [AGENT: Fill this in. What should the next agent work on?] -->
-

---
## Raw Session History
```text
{daily_log}
```
"""
    
    # ... existing file write ...
```

**Update `main()` to wire everything together:**

```python
def main() -> None:
    # ... existing argparse setup with new flags ...
    
    args = parser.parse_args()
    
    # ... existing validation ...
    
    # Process event_type
    event_type = args.event_type or 'chore'
    
    # Process concepts
    concepts = []
    if args.concepts:
        concepts = [c.strip() for c in args.concepts.split(',') if c.strip()]
    
    # Process impacts
    impacts = []
    if args.impacts:
        impacts = [i.strip() for i in args.impacts.split(',') if i.strip()]
    elif args.suggest_impacts or (not args.quiet and not args.impacts):
        # Auto-suggest if flag set or in interactive mode without explicit impacts
        suggestions = suggest_impacts(args.quiet)
        if suggestions and not args.quiet:
            impacts = prompt_for_impacts(suggestions, args.quiet)
        elif suggestions:
            impacts = suggestions
    
    # Create session log with v2.0 schema
    result = create_log_file(
        args.topic, 
        args.quiet, 
        args.source or "",
        event_type=event_type,
        concepts=concepts,
        impacts=impacts
    )
    
    # ... existing changelog handling ...
```

---

### 3.5 Deliverable 5: Auto-Normalization of v1.x Logs

**What:** Automatically treat legacy `type: atom` logs as `type: log` at parse time.

**Why:** Teams with existing v1.x logs shouldn't be blocked from upgrading. Auto-normalization provides seamless backward compatibility without requiring manual migration.

**Implementation:**

**File:** `.ontos/scripts/ontos_generate_context_map.py`

**Modify `scan_docs()` to auto-normalize:**

```python
def scan_docs(root_dirs: list[str]) -> dict[str, dict]:
    """Scans directories for markdown files and parses their metadata.
    
    Includes auto-normalization of v1.x logs to v2.0 schema.
    """
    files_data = {}
    for root_dir in root_dirs:
        if not os.path.isdir(root_dir):
            print(f"Warning: Directory not found: {root_dir}")
            continue
        for subdir, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith('.md'):
                    # ... existing skip pattern checks ...
                    
                    filepath = os.path.join(subdir, file)
                    frontmatter = parse_frontmatter(filepath)
                    
                    if frontmatter and 'id' in frontmatter:
                        doc_id = frontmatter['id']
                        # ... existing validation ...
                        
                        doc_type = normalize_type(frontmatter.get('type'))
                        
                        # =====================================================
                        # V2.0 AUTO-NORMALIZATION
                        # Detect v1.x logs (type: atom, id starts with log_)
                        # and auto-upgrade to v2.0 schema
                        # =====================================================
                        is_legacy_log = (
                            doc_type == 'atom' and 
                            doc_id.startswith('log_')
                        )
                        
                        if is_legacy_log:
                            doc_type = 'log'
                            # Apply sensible defaults for missing v2.0 fields
                            if 'event_type' not in frontmatter:
                                frontmatter['event_type'] = 'chore'  # Safe default
                            if 'impacts' not in frontmatter:
                                frontmatter['impacts'] = []
                            # Remove depends_on (logs don't use it)
                            frontmatter.pop('depends_on', None)
                        
                        files_data[doc_id] = {
                            'filepath': filepath,
                            'filename': file,
                            'type': doc_type,
                            'depends_on': normalize_depends_on(frontmatter.get('depends_on')) if doc_type != 'log' else [],
                            'status': str(frontmatter.get('status') or 'unknown').strip() or 'unknown',
                            # v2.0 fields
                            'event_type': frontmatter.get('event_type'),
                            'concepts': frontmatter.get('concepts', []),
                            'impacts': frontmatter.get('impacts', []),
                            # Track if this was auto-normalized
                            '_normalized_from_v1': is_legacy_log,
                        }
    return files_data
```

**Add a notification for auto-normalized logs:**

```python
def generate_context_map(target_dirs: list[str], quiet: bool = False, strict: bool = False) -> int:
    # ... after scan_docs() ...
    
    # Count auto-normalized logs
    normalized_count = sum(
        1 for data in files_data.values() 
        if data.get('_normalized_from_v1')
    )
    if normalized_count > 0 and not quiet:
        print(f"â„¹ï¸  Auto-normalized {normalized_count} v1.x log(s) to v2.0 schema")
        print("   Run `python3 .ontos/scripts/ontos_migrate_v2.py` to update files permanently")
```

---

### 3.6 Deliverable 6: Optional Migration Script

**What:** Create `ontos_migrate_v2.py` for teams who want explicit v2.0 schema in their files.

**Why:** While auto-normalization handles runtime, some teams prefer their files to reflect the actual schema. This script updates files in place, creating a clean git history of the migration.

**Implementation:**

**File (NEW):** `.ontos/scripts/ontos_migrate_v2.py`

```python
"""Migrate v1.x log files to v2.0 schema.

This script updates log files in place:
- Changes type: atom â†’ type: log
- Adds event_type: chore (safe default)
- Adds impacts: [] (empty, but valid)
- Removes depends_on (if present)

Run with --dry-run to preview changes.
"""

import os
import sys
import argparse
import re

from ontos_config import __version__, LOGS_DIR


def find_legacy_logs(logs_dir: str) -> list[str]:
    """Find log files with v1.x schema (type: atom, id: log_*).
    
    Args:
        logs_dir: Directory to scan.
        
    Returns:
        List of file paths.
    """
    legacy_logs = []
    
    if not os.path.isdir(logs_dir):
        return []
    
    for filename in os.listdir(logs_dir):
        if not filename.endswith('.md'):
            continue
            
        filepath = os.path.join(logs_dir, filename)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check if it's a legacy log
            if not content.startswith('---'):
                continue
                
            # Look for type: atom AND id: log_*
            has_atom_type = bool(re.search(r'^type:\s*atom\s*$', content, re.MULTILINE))
            has_log_id = bool(re.search(r'^id:\s*log_', content, re.MULTILINE))
            
            if has_atom_type and has_log_id:
                legacy_logs.append(filepath)
                
        except Exception:
            continue
    
    return legacy_logs


def migrate_log_file(filepath: str, dry_run: bool = False) -> tuple[bool, str]:
    """Migrate a single log file to v2.0 schema.
    
    Args:
        filepath: Path to the log file.
        dry_run: If True, don't write changes.
        
    Returns:
        Tuple of (success, message).
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        original_content = content
        
        # Step 1: Change type: atom â†’ type: log
        content = re.sub(
            r'^type:\s*atom\s*$',
            'type: log',
            content,
            flags=re.MULTILINE
        )
        
        # Step 2: Add event_type if missing
        if not re.search(r'^event_type:', content, re.MULTILINE):
            # Insert after type: log line
            content = re.sub(
                r'^(type:\s*log)\s*$',
                r'\1\nevent_type: chore',
                content,
                flags=re.MULTILINE
            )
        
        # Step 3: Add impacts if missing
        if not re.search(r'^impacts:', content, re.MULTILINE):
            # Insert after event_type or status line
            if re.search(r'^event_type:', content, re.MULTILINE):
                content = re.sub(
                    r'^(event_type:\s*\w+)\s*$',
                    r'\1\nimpacts: []',
                    content,
                    flags=re.MULTILINE
                )
            elif re.search(r'^status:', content, re.MULTILINE):
                content = re.sub(
                    r'^(status:\s*\w+)\s*$',
                    r'\1\nimpacts: []',
                    content,
                    flags=re.MULTILINE
                )
        
        # Step 4: Remove depends_on if present and empty
        content = re.sub(
            r'^depends_on:\s*\[\s*\]\s*\n',
            '',
            content,
            flags=re.MULTILINE
        )
        
        if content == original_content:
            return True, "No changes needed"
        
        if dry_run:
            return True, "Would migrate"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return True, "Migrated"
        
    except Exception as e:
        return False, f"Error: {e}"


def main() -> None:
    parser = argparse.ArgumentParser(
        description='Migrate v1.x log files to v2.0 schema.',
        epilog="""
This script updates log files from v1.x format to v2.0 format:
  - type: atom â†’ type: log
  - Adds event_type: chore (safe default)
  - Adds impacts: [] (empty list)
  - Removes empty depends_on

The auto-normalization in generate_context_map.py handles this at runtime,
but this script updates your files permanently for a clean git history.
"""
    )
    parser.add_argument('--version', '-V', action='version', version=f'%(prog)s {__version__}')
    parser.add_argument('--dir', type=str, default=LOGS_DIR,
                        help=f'Logs directory to scan (default: {LOGS_DIR})')
    parser.add_argument('--dry-run', '-n', action='store_true',
                        help='Preview changes without writing files')
    parser.add_argument('--quiet', '-q', action='store_true',
                        help='Suppress non-error output')
    args = parser.parse_args()
    
    legacy_logs = find_legacy_logs(args.dir)
    
    if not legacy_logs:
        if not args.quiet:
            print("âœ… No v1.x logs found. All logs are already v2.0 schema.")
        return
    
    if not args.quiet:
        print(f"Found {len(legacy_logs)} v1.x log(s) to migrate:\n")
    
    success_count = 0
    for filepath in legacy_logs:
        success, message = migrate_log_file(filepath, args.dry_run)
        if not args.quiet:
            status = "âœ“" if success else "âœ—"
            print(f"  {status} {os.path.basename(filepath)}: {message}")
        if success and message != "No changes needed":
            success_count += 1
    
    if not args.quiet:
        print()
        if args.dry_run:
            print(f"Would migrate {success_count} file(s). Run without --dry-run to apply.")
        else:
            print(f"Migrated {success_count} file(s) to v2.0 schema.")
            if success_count > 0:
                print("\nNext: Run `python3 .ontos/scripts/ontos_generate_context_map.py` to verify.")


if __name__ == "__main__":
    main()
```

---

### 3.7 Deliverable 7: Unified Initialization Script

**What:** Create `ontos_init.py` that combines "install" (copy scripts) and "initiate" (create structure) into one command.

**Why:** v1.x required users to manually copy files and run separate scripts. This friction reduces adoption. A single `ontos_init.py` makes setup as simple as `git init`.

**Implementation:**

**File (NEW):** `ontos_init.py` (in repository root, not in `.ontos/scripts/`)

```python
#!/usr/bin/env python3
"""Initialize Project Ontos in a project directory.

This script combines installation and initiation into a single command:
1. Creates .ontos/scripts/ directory
2. Copies all Ontos scripts
3. Creates docs/ structure (kernel/, strategy/, product/, atom/, logs/)
4. Creates starter mission.md with template
5. Generates initial context map
6. Prints success message

Usage:
    # From your project root:
    python3 /path/to/project-ontos/ontos_init.py
    
    # Or with custom docs directory:
    python3 /path/to/project-ontos/ontos_init.py --docs-dir documentation
"""

import os
import sys
import shutil
import argparse


# Path to this script (used to find source files)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ONTOS_SCRIPTS_SOURCE = os.path.join(SCRIPT_DIR, '.ontos', 'scripts')
ONTOS_HOOKS_SOURCE = os.path.join(SCRIPT_DIR, '.ontos', 'hooks')


def get_target_project_root() -> str:
    """Get the target project root (current working directory)."""
    return os.getcwd()


def copy_scripts(source_dir: str, dest_dir: str) -> int:
    """Copy Ontos scripts to destination.
    
    Args:
        source_dir: Source scripts directory.
        dest_dir: Destination scripts directory.
        
    Returns:
        Number of files copied.
    """
    if not os.path.isdir(source_dir):
        print(f"Error: Source scripts not found at {source_dir}")
        sys.exit(1)
    
    os.makedirs(dest_dir, exist_ok=True)
    
    count = 0
    for filename in os.listdir(source_dir):
        if filename.endswith('.py'):
            src = os.path.join(source_dir, filename)
            dst = os.path.join(dest_dir, filename)
            if not os.path.exists(dst):
                shutil.copy2(src, dst)
                count += 1
    
    return count


def copy_hooks(source_dir: str, dest_dir: str) -> int:
    """Copy Ontos hooks to destination.
    
    Args:
        source_dir: Source hooks directory.
        dest_dir: Destination hooks directory.
        
    Returns:
        Number of files copied.
    """
    if not os.path.isdir(source_dir):
        return 0
    
    os.makedirs(dest_dir, exist_ok=True)
    
    count = 0
    for filename in os.listdir(source_dir):
        src = os.path.join(source_dir, filename)
        dst = os.path.join(dest_dir, filename)
        if os.path.isfile(src) and not os.path.exists(dst):
            shutil.copy2(src, dst)
            count += 1
    
    return count


def create_docs_structure(project_root: str, docs_dir: str) -> None:
    """Create the documentation directory structure.
    
    Args:
        project_root: Project root directory.
        docs_dir: Name of docs directory (e.g., 'docs').
    """
    docs_path = os.path.join(project_root, docs_dir)
    
    subdirs = ['kernel', 'strategy', 'product', 'atom', 'logs']
    for subdir in subdirs:
        os.makedirs(os.path.join(docs_path, subdir), exist_ok=True)


def create_mission_template(filepath: str) -> bool:
    """Create a starter mission.md file.
    
    Args:
        filepath: Path to create the file.
        
    Returns:
        True if file was created, False if it already exists.
    """
    if os.path.exists(filepath):
        return False
    
    content = """---
id: mission
type: kernel
status: draft
depends_on: []
---

# Project Mission

## Why This Project Exists

<!-- What problem does this project solve? Who has this problem? -->

## Core Principles

<!-- What values guide technical decisions? List 3-5 principles. -->

1. 
2. 
3. 

## Success Looks Like

<!-- How will you know when it's working? What does done look like? -->

"""
    
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return True


def create_template_file(filepath: str) -> bool:
    """Create the _template.md file.
    
    Args:
        filepath: Path to create the file.
        
    Returns:
        True if file was created, False if it already exists.
    """
    if os.path.exists(filepath):
        return False
    
    content = """---
id: _template         # Underscore prefix marks this as internal (excluded from graph)
type: atom            # REQUIRED. Options: kernel, strategy, product, atom, log
status: draft         # Optional. Options: draft, active, deprecated
depends_on: []        # List of dependency IDs (for Space types)
# For log type, also add:
# event_type: chore   # REQUIRED for logs. Options: feature, fix, refactor, exploration, chore
# concepts: []        # Optional tags for searchability
# impacts: []         # REQUIRED for logs. List of Space document IDs affected
---

# Title of Document

Content goes here...
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return True


def generate_initial_context_map(project_root: str) -> bool:
    """Run the context map generator.
    
    Args:
        project_root: Project root directory.
        
    Returns:
        True if successful.
    """
    import subprocess
    
    script_path = os.path.join(project_root, '.ontos', 'scripts', 'ontos_generate_context_map.py')
    
    if not os.path.exists(script_path):
        print("Warning: Could not generate context map (script not found)")
        return False
    
    try:
        result = subprocess.run(
            [sys.executable, script_path, '--quiet'],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.returncode == 0
    except Exception as e:
        print(f"Warning: Could not generate context map: {e}")
        return False


def main() -> None:
    parser = argparse.ArgumentParser(
        description='Initialize Project Ontos in a project directory.',
        epilog="""
Examples:
    # Initialize in current directory with default 'docs/' structure
    python3 /path/to/project-ontos/ontos_init.py
    
    # Use 'documentation/' instead of 'docs/'
    python3 /path/to/project-ontos/ontos_init.py --docs-dir documentation
    
    # Preview what would be created
    python3 /path/to/project-ontos/ontos_init.py --dry-run
"""
    )
    parser.add_argument('--docs-dir', type=str, default='docs',
                        help='Name of documentation directory (default: docs)')
    parser.add_argument('--dry-run', '-n', action='store_true',
                        help='Preview what would be created without making changes')
    parser.add_argument('--force', '-f', action='store_true',
                        help='Overwrite existing files')
    args = parser.parse_args()
    
    project_root = get_target_project_root()
    
    print(f"Initializing Ontos in: {project_root}\n")
    
    if args.dry_run:
        print("DRY RUN - no changes will be made\n")
    
    # Step 1: Copy scripts
    scripts_dest = os.path.join(project_root, '.ontos', 'scripts')
    if args.dry_run:
        print(f"  Would create: {scripts_dest}/")
        print(f"  Would copy scripts from: {ONTOS_SCRIPTS_SOURCE}")
    else:
        count = copy_scripts(ONTOS_SCRIPTS_SOURCE, scripts_dest)
        print(f"  âœ“ Copied {count} script(s) to .ontos/scripts/")
    
    # Step 2: Copy hooks
    hooks_dest = os.path.join(project_root, '.ontos', 'hooks')
    if args.dry_run:
        print(f"  Would create: {hooks_dest}/")
    else:
        count = copy_hooks(ONTOS_HOOKS_SOURCE, hooks_dest)
        if count > 0:
            print(f"  âœ“ Copied {count} hook(s) to .ontos/hooks/")
    
    # Step 3: Create docs structure
    if args.dry_run:
        print(f"  Would create: {args.docs_dir}/kernel/")
        print(f"  Would create: {args.docs_dir}/strategy/")
        print(f"  Would create: {args.docs_dir}/product/")
        print(f"  Would create: {args.docs_dir}/atom/")
        print(f"  Would create: {args.docs_dir}/logs/")
    else:
        create_docs_structure(project_root, args.docs_dir)
        print(f"  âœ“ Created {args.docs_dir}/ structure")
    
    # Step 4: Create mission.md
    mission_path = os.path.join(project_root, args.docs_dir, 'kernel', 'mission.md')
    if args.dry_run:
        print(f"  Would create: {mission_path}")
    else:
        if create_mission_template(mission_path):
            print(f"  âœ“ Created starter {args.docs_dir}/kernel/mission.md")
        else:
            print(f"  - Skipped {args.docs_dir}/kernel/mission.md (already exists)")
    
    # Step 5: Create _template.md
    template_path = os.path.join(project_root, args.docs_dir, '_template.md')
    if args.dry_run:
        print(f"  Would create: {template_path}")
    else:
        if create_template_file(template_path):
            print(f"  âœ“ Created {args.docs_dir}/_template.md")
        else:
            print(f"  - Skipped {args.docs_dir}/_template.md (already exists)")
    
    # Step 6: Generate context map
    if args.dry_run:
        print(f"  Would generate: Ontos_Context_Map.md")
    else:
        if generate_initial_context_map(project_root):
            print(f"  âœ“ Generated Ontos_Context_Map.md")
        else:
            print(f"  âš  Could not generate context map (run manually)")
    
    # Success message
    print()
    if args.dry_run:
        print("Run without --dry-run to apply changes.")
    else:
        print("=" * 50)
        print("âœ… Ontos initialized successfully!")
        print("=" * 50)
        print()
        print("Next steps:")
        print(f"  1. Edit {args.docs_dir}/kernel/mission.md with your project's mission")
        print(f"  2. Say 'Activate Ontos' to your AI agent")
        print()
        print("Optional:")
        print("  - Run `python3 .ontos/scripts/ontos_install_hooks.py` to install git hooks")


if __name__ == "__main__":
    main()
```

---

### 3.8 Deliverable 8: Update Script for Root-Level Files

**What:** Modify `ontos_update.py` to handle root-level files like `ontos_init.py`.

**Why:** Without this, v1.x users running `ontos_update.py` won't receive the new unified init script, which lives in the repository root rather than `.ontos/scripts/`.

**Implementation:**

**File:** `.ontos/scripts/ontos_update.py`

**Add root-level file handling:**

```python
import hashlib

# =============================================================================
# ROOT-LEVEL FILES
# =============================================================================
# Some Ontos files live in the repository root, not .ontos/scripts/
# These need explicit handling during updates

ROOT_LEVEL_FILES = [
    'ontos_init.py',      # Unified initialization (v2.0+)
    'requirements.txt',   # Dependencies (if any)
]


def copy_root_files(source_root: str, dest_root: str, quiet: bool = False) -> int:
    """Copy root-level Ontos files to the target project.
    
    Args:
        source_root: Source Ontos repository root.
        dest_root: Target project root.
        quiet: Suppress output if True.
        
    Returns:
        Number of files copied.
    """
    count = 0
    
    for filename in ROOT_LEVEL_FILES:
        src = os.path.join(source_root, filename)
        dst = os.path.join(dest_root, filename)
        
        if not os.path.exists(src):
            continue
        
        # Check if destination exists and is different
        if os.path.exists(dst):
            with open(src, 'rb') as f:
                src_hash = hashlib.md5(f.read()).hexdigest()
            with open(dst, 'rb') as f:
                dst_hash = hashlib.md5(f.read()).hexdigest()
            
            if src_hash == dst_hash:
                continue  # Already up to date
        
        shutil.copy2(src, dst)
        count += 1
        
        if not quiet:
            print(f"  âœ“ Updated {filename}")
    
    return count
```

**Update `main()` to call `copy_root_files()`:**

```python
def main():
    # ... existing argument parsing ...
    
    # Existing: copy scripts from .ontos/scripts/
    scripts_copied = copy_scripts(source_scripts_dir, dest_scripts_dir, args.quiet)
    
    # NEW: copy root-level files
    root_copied = copy_root_files(SOURCE_ONTOS_ROOT, PROJECT_ROOT, args.quiet)
    
    if not args.quiet:
        total = scripts_copied + root_copied
        print(f"\nâœ… Updated {total} file(s)")
    
    # ... rest of function ...
```

---

### 3.9 Phase 1 Summary

After completing Phase 1, you will have:

| Deliverable | Status | What It Does |
|-------------|--------|--------------|
| Type Definitions | âœ… | Explicit `log` type with `allows_depends_on: False` |
| Event Type Taxonomy | âœ… | Defines 5 event types in config |
| Log Schema Validation | âœ… | Enforces required fields on logs |
| Impacts Validation | âœ… | Verifies impacts with archived log leniency |
| Enhanced Archival | âœ… | Creates v2.0-compliant logs with git log fallback |
| Auto-Normalization | âœ… | Seamless backward compatibility |
| Migration Script | âœ… | Optional explicit file migration |
| Unified Init | âœ… | One-command project setup |
| Update Script | âœ… | Handles root-level files |

**Exit Criteria for Phase 1:**
- [ ] Can initialize a project with `python3 ontos_init.py`
- [ ] `ontos_end_session.py` creates `type: log` documents
- [ ] Validation fails if log is missing `event_type` or `impacts`
- [ ] Validation fails if active log `impacts` references non-existent document
- [ ] Validation shows `[INFO]` (not error) for archived log broken references
- [ ] v1.x logs with `type: atom` + `id: log_*` are auto-normalized
- [ ] `ontos_update.py` copies `ontos_init.py` to target projects
- [ ] All existing tests pass
- [ ] New tests for log validation pass

---

## 4. Phase 2: Visibility (v2.1.x)

Phase 2 enhances the context map to provide better visibility into the knowledge graph.

### 4.1 Deliverable 9: Token Estimation

**What:** Add approximate token counts to each document in the context map.

**Why:** AI agents have limited context windows. Token estimates help agents (and humans) understand the "cost" of loading a document and make better decisions about what to load.

**Implementation:**

**File:** `.ontos/scripts/ontos_generate_context_map.py`

**Add token estimation function:**

```python
def estimate_tokens(content: str) -> int:
    """Estimate token count using character-based heuristic.
    
    Formula: tokens â‰ˆ characters / 4
    
    This is a rough approximation that works well for English text.
    More accurate than word count, simpler than actual tokenization.
    
    Args:
        content: File content as string.
        
    Returns:
        Estimated token count.
    """
    return len(content) // 4


def format_token_count(tokens: int) -> str:
    """Format token count for display.
    
    Args:
        tokens: Token count.
        
    Returns:
        Formatted string (e.g., "~450 tokens" or "~2,100 tokens").
    """
    if tokens < 1000:
        return f"~{tokens} tokens"
    else:
        # Round to nearest 100 for larger counts
        rounded = (tokens // 100) * 100
        return f"~{rounded:,} tokens"
```

**Modify `scan_docs()` to calculate tokens:**

```python
def scan_docs(root_dirs: list[str]) -> dict[str, dict]:
    """Scans directories for markdown files and parses their metadata."""
    files_data = {}
    for root_dir in root_dirs:
        # ... existing code ...
        
        for subdir, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith('.md'):
                    # ... existing code ...
                    
                    filepath = os.path.join(subdir, file)
                    
                    # Read full content for token estimation
                    try:
                        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                            full_content = f.read()
                    except Exception:
                        full_content = ""
                    
                    frontmatter = parse_frontmatter(filepath)
                    
                    if frontmatter and 'id' in frontmatter:
                        # ... existing normalization ...
                        
                        files_data[doc_id] = {
                            # ... existing fields ...
                            'tokens': estimate_tokens(full_content),  # NEW
                        }
    return files_data
```

**Modify `generate_tree()` to include token counts:**

```python
def generate_tree(files_data: dict[str, dict]) -> str:
    """Generates a hierarchy tree string with token estimates."""
    tree = []
    
    # ... existing grouping code ...
    
    for doc_type in order:
        if by_type[doc_type]:
            tree.append(f"### {doc_type.upper()}")
            for doc_id in sorted(by_type[doc_type]):
                data = files_data[doc_id]
                deps = ", ".join(data['depends_on']) if data['depends_on'] else "None"
                tokens = format_token_count(data.get('tokens', 0))
                
                tree.append(f"- **{doc_id}** ({data['filename']}) {tokens}")
                tree.append(f"  - Status: {data['status']}")
                if data['type'] != 'log':
                    tree.append(f"  - Depends On: {deps}")
                else:
                    impacts = ", ".join(data.get('impacts', [])) or "None"
                    tree.append(f"  - Impacts: {impacts}")
            tree.append("")
    
    return "\n".join(tree)
```

---

### 4.2 Deliverable 10: Timeline Section

**What:** Add a "Recent Timeline" section to the context map showing session logs in reverse chronological order.

**Why:** The Timeline provides at-a-glance visibility into what happened during development. New team members can quickly understand recent activity without reading every log file.

**Implementation:**

**File:** `.ontos/scripts/ontos_generate_context_map.py`

**Add Timeline generation function:**

```python
def generate_timeline(files_data: dict[str, dict], max_entries: int = 10) -> str:
    """Generate the Recent Timeline section from log documents.
    
    Args:
        files_data: Dictionary of document metadata.
        max_entries: Maximum number of timeline entries to show.
        
    Returns:
        Formatted timeline string.
    """
    # Extract log documents
    logs = [
        (doc_id, data) for doc_id, data in files_data.items()
        if data['type'] == 'log'
    ]
    
    if not logs:
        return "No session logs found."
    
    # Sort by filename (which starts with date) in reverse order
    logs.sort(key=lambda x: x[1]['filename'], reverse=True)
    
    # Take most recent entries
    recent_logs = logs[:max_entries]
    
    lines = []
    for doc_id, data in recent_logs:
        filename = data['filename']
        event_type = data.get('event_type', 'chore')
        impacts = data.get('impacts', [])
        concepts = data.get('concepts', [])
        
        # Extract date from filename (format: YYYY-MM-DD_slug.md)
        date_part = filename[:10] if len(filename) >= 10 else filename
        
        # Extract title from slug
        slug = filename[11:-3] if len(filename) > 14 else filename[:-3]
        title = slug.replace('-', ' ').replace('_', ' ').title()
        
        # Format line
        line = f"- **{date_part}** [{event_type}] **{title}** (`{doc_id}`)"
        
        if impacts:
            line += f"\n  - Impacted: {', '.join(f'`{i}`' for i in impacts)}"
        
        if concepts:
            line += f"\n  - Concepts: {', '.join(concepts)}"
        
        lines.append(line)
    
    if len(logs) > max_entries:
        lines.append(f"\n*Showing {max_entries} of {len(logs)} sessions*")
    
    return "\n".join(lines)
```

---

### 4.3 Deliverable 11: Provenance Header

**What:** Add a metadata comment at the top of the context map indicating how it was generated.

**Why:** The provenance header helps with debugging stale maps, CI validation, and audit trails.

**Implementation:**

**File:** `.ontos/scripts/ontos_generate_context_map.py`

**Add provenance header function:**

```python
def generate_provenance_header() -> str:
    """Generate metadata header for the context map.
    
    The provenance header provides audit information:
    - Mode: Contributor or User
    - Root: Which directory was scanned
    - Timestamp: When the map was generated (UTC)
    
    Returns:
        HTML comment string with provenance info.
    """
    from ontos_config import DOCS_DIR, PROJECT_ROOT, is_ontos_repo
    
    mode = "Contributor" if is_ontos_repo() else "User"
    
    # Make root path relative
    root = DOCS_DIR
    if root.startswith(PROJECT_ROOT):
        root = root[len(PROJECT_ROOT):].lstrip(os.sep)
    root = root or 'docs'
    
    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
    
    return f"<!-- Generated by Ontos | Mode: {mode} | Root: {root} | {timestamp} -->\n"
```

**Update context map template:**

```python
def generate_context_map(target_dirs: list[str], quiet: bool = False, strict: bool = False) -> int:
    # ... existing code ...
    
    # Generate provenance header
    provenance = generate_provenance_header()
    
    # Generate timeline
    if not quiet:
        print("Generating timeline...")
    timeline = generate_timeline(files_data)
    
    # Build content with new sections
    content = f"""{provenance}# Ontos Context Map
Generated on: {timestamp}
Scanned Directory: `{dirs_str}`

## 1. Hierarchy Tree
{tree_view}

## 2. Dependency Audit
{'No issues found.' if not issues else chr(10).join(issues)}

## 3. Recent Timeline
{timeline}

## 4. Index
| ID | Filename | Type | Tokens |
|---|---|---|---|
"""
    
    for doc_id, data in sorted(files_data.items()):
        tokens = format_token_count(data.get('tokens', 0))
        content += f"| {doc_id} | [{data['filename']}]({data['filepath']}) | {data['type']} | {tokens} |\n"
    
    # ... rest of function ...
```

---

### 4.4 Phase 2 Summary

After completing Phase 2, you will have:

| Deliverable | Status | What It Does |
|-------------|--------|--------------|
| Token Estimation | âœ… | Shows ~token counts in context map |
| Timeline Section | âœ… | Recent session history at a glance |
| Provenance Header | âœ… | Audit trail for generated maps |

**Exit Criteria for Phase 2:**
- [ ] Context map shows token counts next to each document
- [ ] Context map has "## 3. Recent Timeline" section
- [ ] Timeline shows date, event type, title, and impacts
- [ ] Context map starts with `<!-- Generated by Ontos | Mode: ... -->`
- [ ] CI can validate mode from provenance header

---

## 5. Phase 3: Intelligence (v2.2.x)

Phase 3 adds AI-powered features that improve the agent experience.

### 5.1 Deliverable 12: Summary Generation

**What:** Create `ontos_summarize.py` that identifies documents without summaries and generates prompts for 50-word summaries.

**Why:** Agents can read summaries first, then decide whether to load full documents. This reduces context window usage for navigation.

**Implementation:**

**File (NEW):** `.ontos/scripts/ontos_summarize.py`

```python
"""Generate summaries for Ontos documents.

This script reads document content and generates a 50-word summary
that can be stored in the frontmatter or displayed in the context map.

Note: This script requires an LLM to generate summaries. In v2.2,
summaries are generated by the AI agent during "Maintain Ontos".
"""

import os
import sys
import argparse
import re

from ontos_config import __version__, DOCS_DIR


def extract_content(filepath: str) -> str:
    """Extract markdown content (excluding frontmatter).
    
    Args:
        filepath: Path to markdown file.
        
    Returns:
        Content without YAML frontmatter.
    """
    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
        content = f.read()
    
    # Remove frontmatter
    if content.startswith('---'):
        parts = content.split('---', 2)
        if len(parts) >= 3:
            return parts[2].strip()
    
    return content.strip()


def find_documents_needing_summary(docs_dir: str) -> list[tuple[str, str]]:
    """Find documents without summaries in frontmatter.
    
    Args:
        docs_dir: Documentation directory.
        
    Returns:
        List of (filepath, doc_id) tuples.
    """
    needs_summary = []
    
    for root, dirs, files in os.walk(docs_dir):
        for file in files:
            if not file.endswith('.md') or file.startswith('_'):
                continue
            
            filepath = os.path.join(root, file)
            
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            
            if not content.startswith('---'):
                continue
            
            # Check if summary field exists
            if 'summary:' not in content.split('---')[1]:
                # Extract doc_id
                match = re.search(r'^id:\s*(\S+)', content, re.MULTILINE)
                if match:
                    doc_id = match.group(1)
                    # Skip logs (they don't need summaries)
                    if not doc_id.startswith('log_'):
                        needs_summary.append((filepath, doc_id))
    
    return needs_summary


def generate_summary_prompt(filepath: str, doc_id: str) -> str:
    """Generate a prompt for the AI to create a summary.
    
    Args:
        filepath: Path to the document.
        doc_id: Document ID.
        
    Returns:
        Prompt string.
    """
    content = extract_content(filepath)
    
    # Truncate very long content
    if len(content) > 5000:
        content = content[:5000] + "\n\n[Content truncated...]"
    
    return f"""Generate a 50-word summary for this document.

Document ID: {doc_id}
File: {filepath}

Content:
{content}

Requirements:
- Exactly 50 words (Â±5 words is acceptable)
- Focus on WHAT the document describes, not HOW
- Use present tense
- Start with a noun phrase (e.g., "Authentication flow for..." not "This document describes...")
- No markdown formatting in the summary

Output only the summary text, nothing else.
"""


def main() -> None:
    parser = argparse.ArgumentParser(
        description='Generate summaries for Ontos documents.',
        epilog="""
This script identifies documents without summaries and generates prompts
for AI-assisted summary creation.

Workflow:
1. Run this script to find documents needing summaries
2. For each document, the AI generates a 50-word summary
3. Add the summary to the document's frontmatter:
   
   ---
   id: my_doc
   type: atom
   summary: "50-word summary goes here..."
   ---
"""
    )
    parser.add_argument('--version', '-V', action='version', version=f'%(prog)s {__version__}')
    parser.add_argument('--dir', type=str, default=DOCS_DIR,
                        help=f'Documentation directory (default: {DOCS_DIR})')
    parser.add_argument('--file', type=str,
                        help='Generate prompt for a specific file')
    parser.add_argument('--quiet', '-q', action='store_true',
                        help='Suppress non-error output')
    args = parser.parse_args()
    
    if args.file:
        # Single file mode
        if not os.path.exists(args.file):
            print(f"Error: File not found: {args.file}")
            sys.exit(1)
        
        # Extract doc_id from file
        with open(args.file, 'r') as f:
            content = f.read()
        match = re.search(r'^id:\s*(\S+)', content, re.MULTILINE)
        doc_id = match.group(1) if match else os.path.basename(args.file)
        
        print(generate_summary_prompt(args.file, doc_id))
        return
    
    # Find all documents needing summaries
    needs_summary = find_documents_needing_summary(args.dir)
    
    if not needs_summary:
        if not args.quiet:
            print("âœ… All documents have summaries.")
        return
    
    if not args.quiet:
        print(f"Found {len(needs_summary)} document(s) without summaries:\n")
        for filepath, doc_id in needs_summary:
            print(f"  - {doc_id} ({os.path.basename(filepath)})")
        
        print()
        print("To generate summaries, run for each file:")
        print("  python3 .ontos/scripts/ontos_summarize.py --file <filepath>")
        print()
        print("Or ask your AI agent: 'Generate summaries for documents missing them'")


if __name__ == "__main__":
    main()
```

---

### 5.2 Deliverable 13: Auto-Activation Check

**What:** Update Agent Instructions for silent Ontos detection.

**Why:** Instead of requiring users to say "Activate Ontos", agents can silently check for `Ontos_Context_Map.md` and load context automatically.

**Implementation:**

Update `Ontos_Agent_Instructions.md`:

```markdown
### Auto-Activation (v2.2+)

At the start of EVERY conversation, silently check for Ontos:

1. Look for `Ontos_Context_Map.md` in the project root
2. If found AND user hasn't disabled auto-activation:
   - Read the context map
   - Load relevant context based on user's first message
   - Do NOT announce "Loaded: [...]" unless explicitly asked
3. If not found:
   - Proceed normally (no Ontos in this project)

This makes Ontos invisible when present, reducing friction.

**User can disable auto-activation:**
> "Don't auto-load Ontos context" or "Start fresh"
```

---

### 5.3 Phase 3 Summary

After completing Phase 3, you will have:

| Deliverable | Status | What It Does |
|-------------|--------|--------------|
| Summary Generation | âœ… | Script to help generate 50-word summaries |
| Auto-Activation | âœ… | Protocol for silent Ontos detection |

**Note:** The pre-commit hook originally planned for Phase 3 has been removed. The existing pre-push hook is sufficient for archival enforcement without creating friction in the inner development loop.

**Exit Criteria for Phase 3:**
- [ ] `ontos_summarize.py` identifies documents without summaries
- [ ] Agent instructions updated for auto-activation

---

## 6. Testing Strategy

### 6.1 Unit Tests to Add

**File:** `tests/test_log_validation.py`

```python
"""Tests for v2.0 log validation."""

import pytest
from ontos_generate_context_map import validate_log_schema, validate_impacts


class TestLogSchemaValidation:
    """Tests for log-type document validation."""
    
    def test_valid_log_passes(self):
        """Test that a valid v2.0 log passes validation."""
        files_data = {
            'log_20250101_test': {
                'filepath': 'docs/logs/2025-01-01_test.md',
                'filename': '2025-01-01_test.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': 'feature',
                'concepts': ['auth'],
                'impacts': ['auth_flow'],
            }
        }
        
        issues = validate_log_schema(files_data)
        assert len(issues) == 0
    
    def test_missing_event_type_fails(self):
        """Test that log without event_type is flagged."""
        files_data = {
            'log_test': {
                'filepath': 'docs/logs/test.md',
                'filename': 'test.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': None,
                'concepts': [],
                'impacts': [],
            }
        }
        
        issues = validate_log_schema(files_data)
        assert len(issues) == 1
        assert '[MISSING FIELD]' in issues[0]
        assert 'event_type' in issues[0]
    
    def test_invalid_event_type_fails(self):
        """Test that invalid event_type is flagged."""
        files_data = {
            'log_test': {
                'filepath': 'docs/logs/test.md',
                'filename': 'test.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': 'invalid_type',
                'concepts': [],
                'impacts': [],
            }
        }
        
        issues = validate_log_schema(files_data)
        assert len(issues) == 1
        assert '[INVALID VALUE]' in issues[0]
    
    def test_log_with_depends_on_warned(self):
        """Test that log with depends_on gets a warning."""
        files_data = {
            'log_test': {
                'filepath': 'docs/logs/test.md',
                'filename': 'test.md',
                'type': 'log',
                'depends_on': ['something'],
                'status': 'active',
                'event_type': 'feature',
                'concepts': [],
                'impacts': [],
            }
        }
        
        issues = validate_log_schema(files_data)
        assert len(issues) == 1
        assert '[WARNING]' in issues[0]


class TestImpactsValidation:
    """Tests for impacts field validation."""
    
    def test_valid_impacts_pass(self):
        """Test that valid impacts pass validation."""
        files_data = {
            'auth_flow': {
                'filepath': 'docs/atom/auth.md',
                'type': 'atom',
                'depends_on': [],
                'status': 'active',
            },
            'log_test': {
                'filepath': 'docs/logs/test.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': 'feature',
                'impacts': ['auth_flow'],
            }
        }
        
        issues = validate_impacts(files_data)
        assert len(issues) == 0
    
    def test_broken_impact_fails_for_active_log(self):
        """Test that impact referencing non-existent doc is flagged for active logs."""
        files_data = {
            'log_test': {
                'filepath': 'docs/logs/test.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': 'feature',
                'impacts': ['nonexistent'],
            }
        }
        
        issues = validate_impacts(files_data)
        assert len(issues) == 1
        assert '[BROKEN LINK]' in issues[0]
        assert 'nonexistent' in issues[0]
    
    def test_broken_impact_info_for_archived_log(self):
        """Test that broken impact in archived log is INFO, not error."""
        files_data = {
            'log_test': {
                'filepath': 'docs/logs/test.md',
                'type': 'log',
                'depends_on': [],
                'status': 'archived',  # Archived!
                'event_type': 'feature',
                'impacts': ['deleted_doc'],
            }
        }
        
        issues = validate_impacts(files_data)
        assert len(issues) == 1
        assert '[INFO]' in issues[0]  # Not BROKEN LINK
        assert 'no action needed' in issues[0]
    
    def test_impact_to_log_not_allowed(self):
        """Test that impacts cannot reference other logs."""
        files_data = {
            'log_older': {
                'filepath': 'docs/logs/older.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': 'feature',
                'impacts': [],
            },
            'log_newer': {
                'filepath': 'docs/logs/newer.md',
                'type': 'log',
                'depends_on': [],
                'status': 'active',
                'event_type': 'feature',
                'impacts': ['log_older'],  # Invalid: can't impact a log
            }
        }
        
        issues = validate_impacts(files_data)
        assert len(issues) == 1
        assert '[INVALID REFERENCE]' in issues[0]
```

**File:** `tests/test_auto_normalization.py`

```python
"""Tests for v1.x â†’ v2.0 auto-normalization."""

import pytest


class TestAutoNormalization:
    """Tests for automatic normalization of legacy logs."""
    
    def test_legacy_log_normalized_to_v2(self, tmp_path):
        """Test that type: atom + id: log_* becomes type: log."""
        from ontos_generate_context_map import scan_docs
        
        logs_dir = tmp_path / "docs" / "logs"
        logs_dir.mkdir(parents=True)
        
        (logs_dir / "2025-01-01_test.md").write_text("""---
id: log_20250101_test
type: atom
status: active
depends_on: []
---
# Legacy Log
""")
        
        result = scan_docs([str(tmp_path / "docs")])
        
        assert 'log_20250101_test' in result
        assert result['log_20250101_test']['type'] == 'log'
        assert result['log_20250101_test']['event_type'] == 'chore'
        assert result['log_20250101_test']['impacts'] == []
        assert result['log_20250101_test']['_normalized_from_v1'] == True
    
    def test_regular_atom_not_normalized(self, tmp_path):
        """Test that regular atoms are not affected."""
        from ontos_generate_context_map import scan_docs
        
        docs_dir = tmp_path / "docs"
        docs_dir.mkdir()
        
        (docs_dir / "api.md").write_text("""---
id: api_spec
type: atom
status: active
depends_on: []
---
# API Spec
""")
        
        result = scan_docs([str(docs_dir)])
        
        assert 'api_spec' in result
        assert result['api_spec']['type'] == 'atom'  # NOT changed to log
        assert result['api_spec'].get('_normalized_from_v1') != True
```

### 6.2 Integration Tests

```bash
# Test full Phase 1 workflow
cd examples/task-tracker

# 1. Create a v2.0 log
python3 ../../.ontos/scripts/ontos_end_session.py "test-session" \
    --event-type feature \
    --concepts "auth,oauth" \
    --impacts "features" \
    --source "Test"

# 2. Verify log was created with correct schema
cat docs/logs/$(date +%Y-%m-%d)_test-session.md | grep "type: log"
cat docs/logs/$(date +%Y-%m-%d)_test-session.md | grep "event_type: feature"

# 3. Generate context map
python3 ../../.ontos/scripts/ontos_generate_context_map.py

# 4. Verify Timeline section exists
grep -A 10 "## 3. Recent Timeline" ../../Ontos_Context_Map.md

# 5. Verify provenance header
head -1 ../../Ontos_Context_Map.md | grep "Mode: User"
```

---

## 7. Migration & Backward Compatibility

### 7.1 Zero-Breaking-Change Guarantee

The v2.0 implementation MUST maintain full backward compatibility:

| Scenario | Expected Behavior |
|----------|-------------------|
| v1.x project runs v2.0 scripts | Works. Legacy logs auto-normalized. |
| v2.0 project runs v1.5 scripts | Works. v2.0 fields ignored. |
| Mixed v1.x and v2.0 logs in same project | Works. Auto-normalization handles v1.x. |
| User never runs migration script | Works. Auto-normalization at runtime. |
| Space document deleted after log referenced it | Works. Archived logs show INFO, not error. |

### 7.2 Migration Path Options

**Option A: Do Nothing (Recommended)**
- Auto-normalization handles everything
- v1.x logs continue working
- No manual effort required

**Option B: Explicit Migration**
- Run `ontos_migrate_v2.py`
- Files updated in place
- Clean git history of schema change
- Better for teams who want explicit v2.0 in files

**Option C: Gradual Migration**
- New logs created with v2.0 schema
- Old logs remain v1.x (auto-normalized at runtime)
- Eventually all logs become v2.0 organically

### 7.3 What Changes Are Safe

| Change | Safe? | Notes |
|--------|-------|-------|
| Add new fields to log schema | âœ… Yes | Existing code ignores unknown fields |
| Add new validation rules | âœ… Yes | Only affects strict mode |
| Change context map format | âœ… Yes | It's a generated artifact |
| Remove required field | âŒ No | Would break existing v2.0 logs |
| Change event type names | âŒ No | Would invalidate existing logs |

---

## 8. Risk Assessment

### 8.1 Technical Risks

| Risk | Impact | Mitigation |
|------|--------|------------|
| Auto-normalization misidentifies non-logs | Medium | Detection requires BOTH `type: atom` AND `id: log_*` |
| Token estimation inaccurate | Low | Heuristic is clearly documented as approximate |
| Timeline parsing breaks on edge cases | Medium | Graceful fallback to "No sessions found" |
| Migration script corrupts files | High | Always backup; use `--dry-run` first |
| Git log fallback fails | Low | Graceful degradation to empty suggestions |

### 8.2 Process Risks

| Risk | Impact | Mitigation |
|------|--------|------------|
| Users confused by two schema versions | Medium | Clear documentation; auto-normalization hides complexity |
| CI fails on validation after upgrade | Medium | Document that `--strict` will flag old logs |
| Agents don't understand new schema | Medium | Update Agent Instructions before release |

### 8.3 Self-Development Risk (Ontos-Specific)

**Risk:** Modifying `ontos_generate_context_map.py` while using it to document changes.

**Mitigation:**
1. Run tests before committing: `pytest tests/ -v`
2. Run validation after changes: `python3 .ontos/scripts/ontos_generate_context_map.py`
3. Use `--no-verify` for commits that change validation logic
4. Test on `examples/task-tracker/` before main repo

---

## 9. Implementation Checklist

### Phase 1: Structure (v2.0.x)

**Config & Types (R2):**
- [ ] **9.1.1** Add explicit `log` type to TYPE_DEFINITIONS with `allows_depends_on: False`
- [ ] **9.1.2** Add `VALID_TYPES` set to `ontos_config_defaults.py`
- [ ] **9.1.3** Add `EVENT_TYPES` constant to `ontos_config_defaults.py`
- [ ] **9.1.4** Add `VALID_EVENT_TYPES` set to `ontos_config_defaults.py`

**Validation (R3):**
- [ ] **9.1.5** Create `validate_log_schema()` function in `ontos_generate_context_map.py`
- [ ] **9.1.6** Create `validate_impacts()` function with archived log leniency
- [ ] **9.1.7** Add `--strict` flag for CI enforcement
- [ ] **9.1.8** Modify `scan_docs()` to extract `event_type`, `concepts`, `impacts`
- [ ] **9.1.9** Modify `scan_docs()` to auto-normalize v1.x logs
- [ ] **9.1.10** Update `generate_context_map()` to call new validations

**Archival (R1):**
- [ ] **9.1.11** Add `--event-type` flag to `ontos_end_session.py`
- [ ] **9.1.12** Add `--concepts` flag to `ontos_end_session.py`
- [ ] **9.1.13** Add `--impacts` flag to `ontos_end_session.py`
- [ ] **9.1.14** Add `--suggest-impacts` flag to `ontos_end_session.py`
- [ ] **9.1.15** Implement `suggest_impacts()` with git log fallback
- [ ] **9.1.16** Implement `prompt_for_impacts()` function
- [ ] **9.1.17** Update `create_log_file()` to generate v2.0 schema

**Scripts:**
- [ ] **9.1.18** Create `ontos_migrate_v2.py` script
- [ ] **9.1.19** Create `ontos_init.py` script (root level)
- [ ] **9.1.20** Update `ontos_update.py` with root-level file handling (R5)

**Testing & Documentation:**
- [ ] **9.1.21** Add tests for log validation
- [ ] **9.1.22** Add tests for impacts validation (including archived leniency)
- [ ] **9.1.23** Add tests for auto-normalization
- [ ] **9.1.24** Update version to `2.0.0` in `ontos_config_defaults.py`
- [ ] **9.1.25** Update `Ontos_CHANGELOG.md`
- [ ] **9.1.26** Update `Ontos_Agent_Instructions.md`
- [ ] **9.1.27** Update `Ontos_Manual.md`

### Phase 2: Visibility (v2.1.x)

- [ ] **9.2.1** Add `estimate_tokens()` function
- [ ] **9.2.2** Add `format_token_count()` function
- [ ] **9.2.3** Modify `scan_docs()` to calculate tokens
- [ ] **9.2.4** Modify `generate_tree()` to include token counts
- [ ] **9.2.5** Create `generate_timeline()` function
- [ ] **9.2.6** Create `generate_provenance_header()` function
- [ ] **9.2.7** Update context map template with new sections
- [ ] **9.2.8** Add tests for token estimation
- [ ] **9.2.9** Add tests for timeline generation
- [ ] **9.2.10** Update version to `2.1.0`

### Phase 3: Intelligence (v2.2.x)

- [ ] **9.3.1** Create `ontos_summarize.py` script
- [ ] **9.3.2** Update Agent Instructions for auto-activation
- [ ] **9.3.3** Add tests for summary script
- [ ] **9.3.4** Update version to `2.2.0`

---

## Appendix A: File Change Summary

| File | Action | Changes |
|------|--------|---------|
| `ontos_config_defaults.py` | Modify | Add `log` to TYPE_DEFINITIONS, add `allows_depends_on`, add `EVENT_TYPES`, `VALID_EVENT_TYPES`, `VALID_TYPES` |
| `ontos_generate_context_map.py` | Modify | Add validations with archived leniency, token estimation, timeline, provenance, `--strict` flag |
| `ontos_end_session.py` | Modify | Add v2.0 flags, enhanced suggest impacts with git log fallback, v2.0 log template |
| `ontos_update.py` | Modify | Add `ROOT_LEVEL_FILES`, `copy_root_files()` |
| `ontos_init.py` | Create | Unified initialization script (root level) |
| `ontos_migrate_v2.py` | Create | Optional migration script |
| `ontos_summarize.py` | Create | Summary generation helper |
| `tests/test_log_validation.py` | Create | Log validation tests |
| `tests/test_auto_normalization.py` | Create | Auto-normalization tests |
| `Ontos_Agent_Instructions.md` | Modify | Update for v2.0 schema and auto-activation |
| `Ontos_Manual.md` | Modify | Document v2.0 schema and features |
| `Ontos_CHANGELOG.md` | Modify | Add v2.0.0, v2.1.0, v2.2.0 entries |

---

## Appendix B: Schema Quick Reference

### Space Types (v1.x compatible)

```yaml
---
id: unique_id
type: kernel | strategy | product | atom
status: draft | active | deprecated
depends_on: [parent_id1, parent_id2]
---
```

### Time Type (v2.0 new)

```yaml
---
id: log_YYYYMMDD_slug
type: log
status: active | archived
event_type: feature | fix | refactor | exploration | chore
concepts: [tag1, tag2]  # Optional
impacts: [doc_id1, doc_id2]  # Required (can be empty)
---
```

---

## Appendix C: Recommended Execution Order

Based on Gemini's validation, implement in this order:

1. **R2: Config & Types** (`ontos_config_defaults.py`) â€” Defines the schema
2. **R3: Validation** (`ontos_generate_context_map.py`) â€” Builds validation logic with archived leniency
3. **R1: Archival** (`ontos_end_session.py`) â€” Updates user tooling with git log fallback
4. **R5: Distribution** (`ontos_update.py`) â€” Handles root-level files
5. **Init Script** (`ontos_init.py`) â€” Unified initialization

This order ensures each layer builds on the previous one.

---

## Appendix D: Key Architectural Principles

These principles guided the v2.0 design and should guide future development:

1. **Space documents use `depends_on`; Time documents use `impacts`**
   - The `allows_depends_on` field in TYPE_DEFINITIONS encodes this in data, not code

2. **Truth can evolve; History is immutable**
   - Archived logs with broken `impacts` show `[INFO]`, not `[BROKEN LINK]`
   - Deleting a Space document doesn't invalidate historical logs

3. **Auto-normalization over forced migration**
   - v1.x logs work seamlessly; explicit migration is optional
   - Zero-breaking-change guarantee

4. **Configuration over hardcoding**
   - Event types, validation rules, and type definitions live in config
   - Easier to extend without code changes

5. **Friction reduction in the inner loop**
   - No pre-commit hooks; pre-push is sufficient
   - Auto-suggest impacts handles the "commit then archive" workflow

---

*This implementation plan was generated from the v2 strategy and architecture documents, refined through cross-review with Gemini CLI, and consolidated into a single source of truth. It is approved for implementation.*